[ { "title": "Joplin插件开发教程", "url": "/posts/JoplinPluginDevelopment/", "categories": "手册, 开发", "tags": "", "date": "2022-10-15 00:00:00 +0800", "snippet": "Joplin 是一个 Markdown 笔记工具，相比于 Obsidian 而言，Joplin 是通过关系型数据库存储所有的笔记、信息等，而 Obsidian 是基于文件的。技术要求 会写代码，会开发 能够读懂英语，需要阅读官方 API 文档 熟悉 JavaScript 的使用：但不是必须得，使用过其他相似的语言也可以。在开发 Enhancement 插件 之前，我也不太熟悉 JS 熟悉 Typescript 的使用：同上，不是必须的 略懂 React、css：同上，不是必须的，仅在开发有 UI 类的插件需要。甚至可以不使用 React 开发，只是能让你的页面代码整洁不少插件功能分类在插件的最终作用对象上，可以大致分为以下四类。一般情况下，需要多个类型的功能一起实现才能够提供良好的用户使用体验。 Markdown 编辑器插件：比如编辑器中的自动补全相关的插件。任何和编辑器相关的功能，都通过该类实现。包括但不限于： 自动补全：比如 NoteLinkSystem 插件通过 @@ 即可触发自动补全，实现对其他笔记链接的快速插入。 编辑器的实时渲染：比如 Enhancement 插件。可以将 Mermaid, Math 等 Markdown 格式的代码直接显示出渲染后的结果。当实时渲染足够强大时，甚至可以不再需要额外的 PDF 预览窗口。 自定义样式：和 Joplin 提供的自定义样式文件 userchrome.css 类似，但是通过代码级插件可以实现自定义元素的 class 类型，再配合 css 样式，即可实现基本对任意元素自定义样式。比如代码高亮、不同的 Markdown 主题、搜索关键词高亮等等。 对内容的修改：一般与快捷键、工具栏等配合实现对内容的自动修改。比如高亮、加背景色等等。 … Markdown 渲染插件：比如 Admonition 插件。该部分负责修改 Markdown 文本转为 HTML 的过程。包括但不限于： 自定义 Markdown 语法渲染：比如 Admonition 插件能够将 !!!note 渲染成更醒目的格式。 修改现有 Markdown 渲染结果：比如 Enhancement 插件能够将 ![title]() 渲染为带有图标题的结果。 带有独立 Panel 的信息收集、展示、交互等：比如 NoteLink 插件。 Joplin 数据库相关插件：比如 BackUp 插件。修改笔记的文件夹路径、标题，新建/删除笔记等等。插件开发初始化初始化工具Joplin 提供了插件开发开始时的初始化工具 generaor-joplin。具体细节请参考官方网站：Getting started with plugin development。 安装 Node.js npm install -g yo generator-joplin 在任意文件夹内，执行 yo joplin 完成初始化操作。文件结构里面有几个比较关键的文件： plugin.config.json：里面的 extraScripts 用于存放需要转换成 *.js 的 *.ts 以及 *.tsx 文件路径。**所有的路径都是相对于 ./src 目录而言！ 插件开发时，Joplin 的插件接口有时候需要直接提供 *.js 文件路径，但是一般情况下都是用 *.ts 或者 *.tsx 实现的，这种情况下就需要在这个文件中配置一下，不然编译时无法找到对应的 *.js 文件。其它 *.ts 文件之间相互引用的情况则无需在这个文件中配置。 ./src/index.ts：插件的入口，初始化后一般包含以下格式，只替换中间的部分即可。 ./api：这个是包含在初始文件中的、配合 Joplin 插件接口、函数、类等，最好不要修改。joplin.plugins.register({\tonStart: async function() { // 从这里开始写你的插件逻辑 }})注册插件await joplin.contentScripts.register( ContentScriptType.MarkdownItPlugin, // 或者是 ContentScriptType.CodeMirrorPlugin, 分别对应 markdown 渲染插件以及 markdown 编辑器插件 '唯一的插件ID', './插件的路径/index.js' // 这里的文件如果是用 ts 写的，那么需要再 plugin.config.json 文件中配置);更细致的内容请参考：joplin开发文档Markdown 编辑器插件开发截止到目前，Joplin 桌面端 的 Markdown 编辑器是基于 Codemirror 5 开发的，目前还没有听到要切换到 Codemirror 6 的消息（Obsidian 等用的就是 v6，v6 相较于 v5 而言新增了一些功能，能够更方便的创建 decoration 等，实现 live preview 功能）。安卓端的 Joplin 使用的则是 v6 版本，但是目前安卓版不支持插件，因此这里仅讨论 v5 版本下的插件开发。index.ts 文件的主体内容：module.exports = { default: function(context) { return { plugin: function(CodeMirror) { // 插件逻辑部分 // 这里是通过 defineOption 方式实现 cm 插件 CodeMirror.defineOption('option的唯一ID', false, async function(cm, val, old) { // xxx } }, codeMirrorResources: [ // cm5 自带的 addons，cm5 本身就自带了不少的插件，比如搜索高亮等 // 具体参考：https://codemirror.net/5/doc/manual.html#addons // 下面的例子对应 mode/overlay.js 以及 hint/show-hint.js 'addon/mode/overlay', 'addon/hint/show-hint' ], codeMirrorOptions: { // 必须在这里进行配置开启，否则无法生效 '对应上面 option的唯一ID': true, }, assets: { // 配置 css 样式文件。Joplin 中 cm5 不允许携带 *.js 脚本 }, } }}Markdown 渲染插件开发独立 Panel 插件开发数据库相关开发注意事项第三方库的使用由于 Joplin 需要跨平台，它自带了一套自己的 fs 以及 sqlite3 库，需要通过以下方式引入包，不能直接 require：const fs = joplin.require('fs-extra')const sqlite3 = joplin.require('sqlite3')这进一步导致所有依赖了 fs 以及 sqlite3 的库都无法正常使用！" }, { "title": "知识获取", "url": "/posts/GetKnowledge/", "categories": "手册, 研究", "tags": "", "date": "2022-08-28 00:00:00 +0800", "snippet": "在信息爆炸的今天，高效的获取高质量数据能够节省大量时间，同时合理的知识处理能够更好的为个人的工作、学习等打下基础。知识获取首先是知识来源的选择，取决于各自的目标，包括但不限于： 微信公众号 InfoQ 的各类 topic：https://www.infoq.cn/topic/ 博客 各论文期刊 其它不知道有哪些时，多和身边的人沟通即可。同时，可以看到信息的来源多种多样，得益于 RSS，我们能够让信息主动来找我们，避免了反复在各种软件、设备中横跳获取信息。然而国内网站对 RSS 支持十分糟糕，因此需要借助第三方软件/平台等让原本不支持 RSS 订阅的网站、公众号上发布的信息能够以 RSS 的形式获取到。 RSSHub：社区共同维护的工具，用来从原本不支持 RSS 的网站生成 RSS werss：专注于微信公众号的 RSS 生成RSS 阅读器则多种多样，任一款自己喜欢的即可。 Newsblur FreshRss Github rss-reader知识处理%%{init: { 'theme': 'neutral' } }%%flowchart LR 来源[各大知识来源] 偶然[偶然得知] RSS(RSS阅读器) RIL(Read-it-later工具) 有价值{似乎有价值?} 有时间{当前有时间?} 阅读([阅读]) 来源--&gt;|rss|RSS--&gt;有价值 有价值--&gt;|Yes|有时间--&gt;|Yes|阅读 有时间--&gt;|No|RIL--&gt;阅读 偶然--&gt;有价值%%{init: { 'theme': 'neutral' } }%%flowchart LR 阅读([阅读]) 有价值{有价值?} 二级(二级知识库收录) 一级(一级知识库蒸馏) 阅读--&gt;有价值--&gt;|Yes|二级--&gt;一级 一级注重知识点与知识点的关系及自己的思考总结，用于泛型知识，及发散思考，注重全局观 二级注重细节知识的积累，用于解决具体问题 检索时统一检索" }, { "title": "ReadCube Papers API 记录", "url": "/posts/ReadCubePapersAPI/", "categories": "开发", "tags": "", "date": "2022-05-21 00:00:00 +0800", "snippet": "本文用于记录在开发 Joplin 的 ReadCube Papers 插件以实现笔记软件与文献管理软件的联动时，使用到的一些 web 版 papers 的接口。插件地址：joplin-plugin-enhancement。登录及 cookie 获取 URL：https://services.readcube.com/authentication/login 方法：POST 请求数据类型为 formData，包含： client: webapp api: // 可为空 client_version: // 可为空 email: 用户邮箱 password: 用户密码 返回值 set-cookie 有三条，分别为： _readcube-login_token _readcube_session user_web_token 三个一起即为 cookie。 WebSocket用于接收来自服务器端关于数据变更的消息，进而实现实时同步。地址：wss://push.readcube.com/bayeux id：每一条客户端发往服务端的消息均会有一个 id 为 36 进制。 需要定期向服务端发送心跳包，否则一段时间后服务器将主动断开 ws 连接 发送/接收的所有数据均为 Json 格式 clientId 很关键，不然无法成功握手，需要先获取 clientIdWebSocket 握手 URL：https://push.readcube.com/bayeux?message=[{\"channel\":\"/meta/handshake\",\"version\":\"1.0\",\"supportedConnectionTypes\":[\"websocket\",\"eventsource\",\"long-polling\",\"cross-origin-long-polling\",\"callback-polling\"],\"id\":\"1\"}]&amp;jsonp=__jsonp1__ 方法：GET 返回：内含有 clientId 及是否成功，且该次交互 id 为 1。 需要设置 CookieWebSocket 连接 示例见：ReadCube Papers WebSocket.ts 订阅： users：[{\"channel\":\"/meta/subscribe\",\"clientId\":\"client-6774640\",\"subscription\":\"/production/users/user-id\",\"id\":\"3\"}]。不知道为啥官方连续请求了 2 次 collections: [{\"channel\":\"/meta/subscribe\",\"clientId\":\"client-6774640\",\"subscription\":\"/production/collections/collection-id/changes\",\"id\":\"4\"}]。不知道为啥官方连续请求了 3 次 心跳包：[{\"channel\":\"/meta/connect\",\"clientId\":\"client-6774640\",\"connectionType\":\"websocket\",\"id\":\"b\"}]。发送后，服务端约 25s 后返回成功结果，此时需要立刻发送下一个心跳包。服务端返回的结果中包含响应的延时长度（现在是 25000），可以以其为依据判断 WebSocket 的存活。信息获取 均需要设置 cookie。示例见：ReadCube Papers.ts可能是多论文库的原因，每个用户有至少一个 collections，每个 collection 中有若干 item，每个item 即为一篇论文，每个 collection 具备唯一 id，item 也具备唯一 id。 功能 URL 方法 说明 获取所有 collections https://sync.readcube.com/collections/ GET   获取指定 collection 中的文章 item 列表 https://sync.readcube.com/collections/{collectionId}/items?sort%5B%5D=title,asc&amp;size=50 GET 一次最多拉取的 size 为50，返回值中有 scroll_id，想要获取后面的需要增加参数 scoll_id 继续调用该接口 获取指定 item 的信息 https://sync.readcube.com/collections/{collectionId}/items/{itemId} GET 上一个 item 列表中也包含具体 item 的信息，该部分应该主要用于同步单一 item 的信息 获取指定 item 的所有 annotation https://sync.readcube.com/collections/{collectionId}/items/{itemId}/annotations GET PDF 里做的标记、笔记等 获取指定 item 的 metadata https://services.readcube.com/reader/metadata?doi=${doi} GET 格式化数据，包括引用、图片、作者信息等 还有许多其它接口，个人需求原因不需要其他接口，因此需要自行研究。跳转 直接浏览器跳转到 PDF：https://www.readcube.com/library/${collectionId}:${itemId} 直接浏览器跳转到 annotation：https://www.readcube.com/library/${collectionId}:${itemId}#annotation:${annotationId}更新item 信息更新统一通过 PATCH 类型请求进行。 URL：https://sync.readcube.com/collections/${collectionId}/items/${itemId}?client=webapp&amp;client_id=${clientId}&amp;client_version=4.12.6 类型：PATCH 参数：{“item”:{“user_data”:{“notes”:”cas”}}} 返回值：item 的全部信息其它的类似。" }, { "title": "数学优化模型分类", "url": "/posts/MathOptimizationModel/", "categories": "手册, 研究", "tags": "数学", "date": "2022-04-09 12:54:07 +0800", "snippet": "在云计算、边缘计算、雾计算等领域，优化问题十分常见，为方便后续研究工作，这里记录一些经常出现的数学模型以方便后续求解问题时的快速查阅，每个模型后的解释均来自引用论文。 目前仍在完善中，之前看的论文没有做记录，后续阅读的论文会不断扩充本篇内容 引用的论文属于建模出的模型符合该类，并不是该类模型第一次出现、被定义的论文；被引用文章的出现与否取决于我个人的阅读到它们的顺序 😃 引用论文主要来源： TCC Integer Programming (IP)Integer Linear Programming (ILP)定义\\(\\begin{array}{ll}\t\\operatorname{maximize} &amp; \\mathbf{c}^{\\mathrm{T}} \\mathbf{x} \\\\\t\\text { subject to } &amp; A \\mathbf{x} \\leq \\mathbf{b} \\\\\t&amp; \\mathbf{x} \\geq \\mathbf{0} \\\\\t&amp; \\mathbf{x} \\in \\mathbb{Z}^{n}\\end{array}\\) 现有成熟的工具求解 ILP 及 MILP 问题，比如 CPLEX 等。 Lenstra et al.1 have proved that no algorithm is able to find a solution in polynomial time with a smaller approximation ratio than 3/2.实例 通过近似算法进行求解，需要考察近似率（Approximation rate）2。Mixed Integer Programming (MIP)ILP 中仅有部分变量具有整数约束，其余变量为实数的情况下为 MIPTwo Stage Mixed Integer Programming\\[\\begin{array}{clc}\t\\min _{z, x, v} &amp; f_{1}(z)+f_{2}(x)+f_{3}(v) &amp; \\\\\t\\text { s.t. } &amp; B_{1} z+B_{2} x &amp; \\leq b_{2} \\\\\t&amp; B_{3} v &amp; \\leq b_{3} \\\\\t&amp; A_{1} z+A_{2} x+A_{3} v &amp; \\leq b_{1} \\\\\t&amp; z \\in \\mathbb{R}^{n}, x \\in \\mathbb{Z}^{m}, v \\in \\mathbb{R}^{p},\t\\end{array}\\]where Z is the set of integers. The first stage variables are z and x, and the second stage variables are v. The first and second stage constraints which are independent of each other are represented by the first two inequalities, while the constraints which couple the first and second stages are represented by the third inequality3.Mixed Integer Non-Linear Programming (MINLP)定义\\[\\begin{aligned}&amp;\\min &amp;&amp;f(x, y)\\\\&amp;\\text { s.t. } &amp;&amp;c_{i}(x, y)=0 \\quad \\forall i \\in E\\\\&amp; &amp;&amp;c_{i}(x, y) \\leq 0 \\quad \\forall i \\in I\\\\&amp; &amp;&amp;x \\quad \\in X\\\\&amp; &amp;&amp;y \\quad \\in Y \\qquad\\text { integer }\\end{aligned}\\]where each $c_i(x,y)$ is a mapping from $R^n$ to $R$, and $E$ and $I$ are index sets for equality and inequality constraints, respectively. Typically, the functions $f$ and $c_i$ have some smoothness properties, i.e., once or twice continuously differentiable4.Software developed for MINLP has generally followed two approaches4: Outer Approximation/Generalized Bender’s Decomposition: These algorithms alternate between solving a mixed-integer LP master problem and nonlinear programming subproblems. Branch-and-Bound: Branch-and-bound methods for mixed-integer LP can be extended to MINLP with a number of tricks added to improve their performance.实例 将某个约束条件暂时不考虑的情况下，将约束中的变量分为互不干扰的两组，进而将原问题分解为主从问题5。 优化问题建模是 MINLP，放宽约束进行了分解，但是之后疑似未还原约束或证明问题相等，使得提出的解法是具备前提条件的：We first consider the scenario that ESs do not cache the relevant services, and generate an initial computing strategy that satisfies the above constraints. 引用 J. K. Lenstra, D. B. Shmoys, and ́E. Tardos, “Approximation algo-rithms for scheduling unrelated parallel machines,”Mathematicalprogramming, vol. 46, no. 1, pp. 259–271, 1990. &#8617; Szalay, M., Matray, P. &amp; Toka, L. Real-Time FaaS: Towards a Latency Bounded Serverless Cloud. Ieee Transactions Cloud Comput PP, 1–1 (2022). &#8617; Sehloff, David Karl, Maitreyee Marathe, Ashray Manur, and Giri Venkataramanan. “Self-sufficient participation in cloud-based demand response.” IEEE Transactions on Cloud Computing (2021). &#8617; https://neos-guide.org/content/mixed-integer-nonlinear-programming &#8617; &#8617;2 H. Zhou, Z. Zhang, D. Li and Z. Su, “Joint Optimization of Computing Offloading and Service Caching in Edge Computing-based Smart Grid,” in IEEE Transactions on Cloud Computing, doi: 10.1109/TCC.2022.3163750. &#8617; " }, { "title": "🔐 Softether VPN 搭建及使用", "url": "/posts/SoftetherVPN/", "categories": "手册, Linux", "tags": "Linux, 软件", "date": "2022-04-06 21:10:30 +0800", "snippet": "在一些诸如公司、学校实验室、部分家用网络等内网环境下，部分服务器、NAS、Emby等由于没有公网 IP 导致离开内网环境下无法访问，此时需要搭建 VPN、端口转发等方式来实现内网外访问。考虑到一些 frp 等端口转发工具不关注用户权限、zerotier 及 n2n 等又不能细粒度的控制用户请求，这里选用功能较为全面的 Softether VPN。 如果只是个人使用，仅想访问内网对应服务且不需要权限管理等额外功能，那么推荐使用 frp、zerotier 等工具，配置简易，使用简单，容易上手。准备一台具备公网 IP 的服务器：Softether 服务端需要能被所有客户端访问到 CPU、RAM 配置无需太高 网络环境要好，和 VPN 的使用体验直接挂钩Softether 服务端安装 现有很多较为完善的教程，直接参考即可。例如：https://www.mivm.cn/softether-vpn-lan/ 官方有图形客户端，无需全程命令行重点功能 VPN 流量等权限管控：通过为不同用户建立不同用户组，并限制不同用户组使用 VPN 的上行、下行等实现权限控制 用户行为监控：有丰富的日志功能记录流量使用情况，并可以记录每个用户具体的数据量、活动连接、来源 IP 等等 安全：支持多账号，支持白名单、黑名单等Softether 客户端连接通用方法使用系统自带 VPN 功能，选择 L2TP/IPsec 类型： 不推荐 Windows 使用此方法 Windows：系统设置 -&gt; 网络和 Internet -&gt; VPN -&gt; 添加 VPN Linux：系统设置 -&gt; 网络 -&gt; VPN MacOS：系统设置 -&gt; 网络 -&gt; 新建 -&gt; VPN 移动设备：系统设置 -&gt; 搜索 VPNWindows 推荐直接使用官方提供的图形客户端 使用通用方法之后，如果发现无法正常上网，需要： 控制面板 -&gt; 网络和 Internet -&gt; 网络连接 里找到刚刚的网络适配器，双击，然后选择 属性 -&gt; 网络 -&gt; TCP/IPv4 -&gt; 高级 -&gt; 取消勾选“在远程网络上使用默认网关”Linux命令行连接 官网下载 Linux 客户端（似乎只有源码包） 解压后，进入 ./vpnclient 目录，执行 make，确保执行完成后，生成 vpnclient 及 vpncmd 两个可执行文件 启动 vpn 服务：sudo ./vpnclient start，强烈建议直接使用 root 权限执行 配置 vpn：./vpncmd 依次选择 client -&gt; localhost help 可查看所有命令列表 AccountCreate 创建新连接，参数和附件指南中一致即可，注意这一步需要输入虚拟网卡名称，没有的话一般会自动创建，如果没有自动创建，那么执行 NicXXXXX 命令手动创建一个网卡 AccountPasswordSet 设置连接的密码 AccountConnect 连接即可 配置 IP 地址 4.3 中创建的虚拟网卡名加上 vpn_ 前缀才是真正的虚拟网卡名称 sudo ifconfig 虚拟网卡名 10.1.1.XXX netmask 255.0.0.0 &lt;- 注意修改 IP 地址 sudo dhclient 虚拟网卡名 &lt;- 可能要执行几十秒甚至一分钟，等着就好 自启动 注意修改自启脚本里的连接名：CONNECTION、虚拟网卡名 及 固定 IP 地址！ 新建 /etc/init.d/vpnclient: #!/bin/sh # Start/stop the vpnclien daemon # ### BEGIN INIT INFO # Provides: vpnclient # Required-Start: $local_fs $syslog $time # Required-Stop: $local_fs $syslog $time # Should-Start: $network # Should-Stop: $network # Default-Start: 2 3 4 5 # Default-Stop: # Short-Description: vpn client service # Description: Softether vpn client service. ### END INIT INFO EXE_DIR=/opt/vpnclient/ SER=\"$EXE_DIR\"vpnclient CMD=\"$EXE_DIR\"vpncmd start() { echo start $SER start $CMD localhost /client /cmd accountconnect CONNECTION # &lt;======= 连接名 ifconfig vpn_xxx 10.1.1.XXX netmask 255.0.0.0 # &lt;=============== 注意修改 IP 地址和虚拟网卡名称 echo end } stop() { $SER stop } case \"$1\" in start) start ;; stop) stop ;; esac exit 0 sudo chmod 755 /etc/init.d/vpnclient sudo systemctl daemon-reload sudo systemctl enable vpnclient.service sudo systemctl start vpnclient.serviceGUI参考“通用方法”。固定 IP 设置 Windows：在网络适配器中修改 IP4 地址 Linux：命令行修改对应适配器的 IP4 地址 MacOS：vpn 高级设置中直接指定 IP4 地址" }, { "title": "dde-grand-search 插件开发", "url": "/posts/dde-grand-search-plugin-Tutorial/", "categories": "开发, Deepin", "tags": "Linux, Deepin", "date": "2022-04-01 14:44:05 +0800", "snippet": "截止到目前 2022-04-01 为止，github 上没有放出 dde-grand-search 的代码仓库，但是可以通过 apt source 的方式下载到全部代码。因此目前没有官方文档展示其插件开发过程，但是当前 Deepin V20.5 版本下，dde-control-center 的搜索效果是以插件形式集成到 dde-grand-search 中的，因此可以参照 dde-control-center 相关部分进行模仿开发。 dde-grand-search 的插件机制可能会随着版本号变化而变动，使得本篇内容不再适用，此时需要按照本篇思路重新梳理插件开发方法。插件文件目录按照软件设计的基本逻辑推断，再结合 dde-dock 中的插件机制，可以推测得到 dde-grand-search 有很大概率也是通过插件的方式进行后续功能增加的，尤其是目前 dde-grand-search 的搜索项目太少，参考 MacOS 的聚焦功能，能够想象到的后续拓展功能就包括：字典、剪贴板、邮件以及其他各种 Deepin 全家桶功能的集成。因此只需要确定插件目录下有哪些以及存在的插件，然后去看对应部分的源码即可。插件目录在没有官方文档的情况下，最简单可靠的方法就是阅读源码，不过首先需要确保 /etc/apt/sources.list 中 deb-src 一行没有被注释掉：apt source dde-grand-search解压代码，Clion 打开项目目录，通过文件名判断插件相关的主要类为 pluginmanager.cpp 中的 PluginManagerPrivate，其中的 defaultPath 即为插件路径。然后利用 Clion 的全局搜索功能，可以查到路径定义在 CMakeLists.txt 中。最终得到路径：/usr/lib/x86_64-linux-gnu/dde-grand-search-daemon/plugins/searcher，同时发现已存在一个插件文件：com.deepin.dde-grand-search.dde-control-center-setting.conf，名称可知是设置中心的插件，内容如下：[Grand Search]Name=com.deepin.dde-grand-search.dde-control-center-settingMode=TriggerDBusService=com.deepin.dde.ControlCenterDBusAddress=/com/deepin/dde/ControlCenterDBusInterface=com.deepin.dde.ControlCenter.GrandSearchInterfaceVersion=1.0显然，是通过配置文件告诉 dde-grand-search 对应插件的 DBus Interface 地址，然后统一调用的。Mode 还有其它模式，现在是搜索时触发模式，其它可以直接搜索源码，对应文件中有着注释。接口及数据格式由上一节可知，实现插件需要两步： 配置文件告诉 dde-grand-search 插件的 DBus 接口 实现配置文件中的接口但是需要实现哪些具体接口，在没有官方文档的情况下，直接参照 dde-control-center 的源码，在官方 github 仓库里简单的搜索 GrandSearch 关键词，即可快速定位到相关内容，主要关键文件如下： src/frame/dbuscontrolcenterservice.h src/frame/dbuscontrolcenterservice.cpp src/frame/window/mainwindow.cpp阅读源码后，可以发现几个关键的 DBus 接口：class DBusControlCenterGrandSearchService: public QDBusAbstractAdaptor{/* ... */public Q_SLOTS: // METHODS // dde-grand-search 输入时，调用该接口获取搜索结果 QString Search(const QString json); bool Stop(const QString json); // 搜索结果被点击时，调用该接口进行响应 bool Action(const QString json);/* ... */接着只需明确输入的 json 和返回的 QString 具体格式即可，参见 MainWindow::GrandSearchSearch()。 以下列出的数据结构不全，仅列出了关键的部分，更为详细的需要翻阅 dde-grand-search 源码Search{ \"ver\": \"应该是版本号，具体类型需要看代码\", \"mID\": \"推测用来标记插件搜索请求的，具体类型看代码\", \"cont\": \"搜索关键词\"}{ \"ver\": \"直接使用输入的 ver\", \"mID\": \"直接使用输入的 mID\", \"cont\": [ { \"group\": \"dde-grand-search 显示结果时的组名称\", \"items\": [ { \"item\": \"搜索结果 QString，触发时将作为传递给 Action 接口的参数\", \"name\": \"搜索结果显示出的字符串，可以和 item 不一样\", \"icon\": \"搜索结果条目图标路径\", \"type\": \"类型，目前来看好像没有什么作用\" }, /* 可以有多个搜索结果 */ ] } ]}Stop该部分此处不涉及，跳过。Action{ \"action\": \"动作类型，我们主要关注 openitem 类型，表示双击了搜索结果\", \"item\": \"选中搜索结果的 item 属性，和 Search 的返回结果中的 item 对应\"}例子：`dde-top-panel`支持搜索菜单 项目地址：https://github.com/SeptemberHX/dde-top-panel/tree/dde-grand-search插件配置文件新建文件 /usr/lib/x86_64-linux-gnu/dde-grand-search-daemon/plugins/searcher/com.deepin.dde-grand-search.dde-top-panel-setting.conf：[Grand Search]Name=com.deepin.dde-grand-search.dde-top-panel-settingMode=TriggerDBusService=com.septemberhx.dde.TopPanelDBusAddress=/com/septemberhx/dde/TopPanelDBusInterface=com.septemberhx.dde.TopPanel.GrandSearchInterfaceVersion=1.0`DBus` 接口实现 DBus 注册等基础部分不包含在下面QString DBusTopPanelService::Search(const QString json) { //解析输入的json值 QJsonDocument jsonDocument = QJsonDocument::fromJson(json.toLocal8Bit().data()); if(!jsonDocument.isNull()) { QJsonObject jsonObject = jsonDocument.object(); QJsonObject jsonResults; QJsonArray items; // 调用具体方法获取菜单搜索结果 QStringList searchResult = this-&gt;parent()-&gt;GrandSearchSearch(jsonObject.value(\"cont\").toString()); // 将搜索结果拼接成 json 格式 for (int i = 0; i &lt; searchResult.length(); i++) { QJsonObject jsonObj; jsonObj.insert(\"item\", searchResult[i]); // 去掉 &amp; 符号，否则影响搜索结果显示的观感 jsonObj.insert(\"name\", searchResult[i].remove('&amp;')); jsonObj.insert(\"icon\", \"menu\"); jsonObj.insert(\"type\", \"application/x-dde-top-panel-xx\"); items.insert(i, jsonObj); } QJsonObject objCont; objCont.insert(\"group\", \"TopPanel\"); objCont.insert(\"items\", items); QJsonArray arrConts; arrConts.insert(0, objCont); jsonResults.insert(\"ver\", jsonObject.value(\"ver\")); jsonResults.insert(\"mID\", jsonObject.value(\"mID\")); jsonResults.insert(\"cont\", arrConts); QJsonDocument document; document.setObject(jsonResults); return document.toJson(QJsonDocument::Compact); } return QString();}bool DBusTopPanelService::Stop(const QString json) { return true;}bool DBusTopPanelService::Action(const QString json) { QString searchName; QJsonDocument jsonDocument = QJsonDocument::fromJson(json.toLocal8Bit().data()); if(!jsonDocument.isNull()) { QJsonObject jsonObject = jsonDocument.object(); if (jsonObject.value(\"action\") == \"openitem\") { // 打开 item 的操作 searchName = jsonObject.value(\"item\").toString(); // 调用结构执行 return this-&gt;parent()-&gt;GrandSearchAction(searchName); } } return false;}实现效果" }, { "title": "强化学习第二版导图（三）：Planning", "url": "/posts/RL-MindMap-3/", "categories": "学习, Reinforcement Learning", "tags": "强化学习, 思维导图", "date": "2022-03-28 22:04:43 +0800", "snippet": "本系列为 Sutton &amp; Barto -《Reinforcement Learning: An Introduction 2nd Edition》的个人读后思维导图记录，便于后续快速复习及回忆，本章对应原文的第 8 章。原书地址：incompleteideas.net，右键新标签查看图片原图。" }, { "title": "常用 shell 命令", "url": "/posts/DailyShellCommands/", "categories": "手册, Linux", "tags": "Linux, shell", "date": "2022-03-21 21:17:17 +0800", "snippet": "Kubernetes# 0. Ubuntu 20.04 一键安装 kubectl kubeadm etcd 等 kubernetes 相关内容sh -c \"$(curl -fsSL https://raw.githubusercontent.com/SeptemberHX/scripts/master/one-key-k8s-ubuntu.sh)\"# 1. 清理命名空间 hx-test 下所有名称含有 service 的 podkubectl get pods -n hx-test | grep service | awk -F ' ' '{print $1}' | xargs kubectl delete pods -n hx-test# 2. 允许 master 节点运行 podkubectl describe node t0 | grep Taints # 查看 taintskubectl taint node t0 node-role.kubernetes.io/master:NoSchedule- # 取消 NoSchedule# 3. 给 node 打标签kubectl label nodes t0 node=node0 # 给节点 t0 打上标签：node=node0kubectl get nodes --show-labels # 查看节点标签# 4. token 过期后还需要添加新节点kubeadm token create --print-join-commandDocker# 1. 给用户 user 操作 docker 权限sudo usermod -aG docker user# 2. 执行一个运行中容器 0e45245944d4 的 bashdocker exec -it 0e45245944d4 /bin/bash# 3. 删除包含 service 的镜像docker image list | grep service | awk -F ' ' '{print $1}' | xargs docker image rm# 4. 删除所有未使用的 volume，image、container 类似docker volume pruneNvidia# 1. 列出所有正在使用 NVIDIA GPU 的进程信息nvidia-smi | grep -A 100 \"PID\" | tail -n +3 | sed '$d' | grep -v 'No' | grep -v 'N/A' | awk -F ' ' '{print $3}' | xargs --no-run-if-empty ps -ux日志# 1. 获取指定 service 的日志sudo journalctl -u emby-server.service" }, { "title": "强化学习第二版导图（二）：时序差分及Bootstrapping", "url": "/posts/RL-MindMap-2/", "categories": "学习, Reinforcement Learning", "tags": "强化学习, 思维导图", "date": "2022-03-18 13:00:29 +0800", "snippet": "本系列为 Sutton &amp; Barto -《Reinforcement Learning: An Introduction 2nd Edition》的个人读后思维导图记录，便于后续快速复习及回忆，本章对应原文的第6-7章。原书地址：incompleteideas.net，右键新标签查看图片原图。" }, { "title": "Kubernetes 集群搭建", "url": "/posts/K8sClusterTutorial/", "categories": "", "tags": "Linux, Kubernetes", "date": "2022-03-15 17:57:19 +0800", "snippet": " 可以看看 Kubernetes 实践指南k8s 搭建流程基础工具及 k8s 安装CentOSyum install git -ygit clone https://github.com/SeptemberHX/scripts.gitcd scripts./basic_utils.shgit clone https://github.com/SeptemberHX/scripts.git # 注意版本号格式./k8s_install.sh 1.13.1-0Ubuntush -c \"$(curl -fsSL https://raw.githubusercontent.com/SeptemberHX/scripts/master/one-key-k8s-ubuntu.sh)\"初始化 master 节点kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.13.1 --apiserver-advertise-address=IPgcr.io 无法访问时，拉取 k8s 镜像的替代方案./k8s_gxrcio.sh v1.13.1 # 注意版本号格式安装 flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml开放 http api 以允许外部操作集群官网文档推荐使用 kubectl proxy，从安全因素考量kubectl proxy --port=8082 --address='0.0.0.0' --accept-hosts='^*$' &amp;注意 --accept-hosts ，否则可能出现 forbidden 错误。问题kubelet isn't running or healthy原问题链接：https://stackoverflow.com/questions/52119985/kubeadm-init-shows-kubelet-isnt-running-or-healthy解决：/etc/docker/daemon.json 文件中添加一下内容：{ \"exec-opts\": [\"native.cgroupdriver=systemd\"]}之后执行命令：sudo systemctl daemon-reloadsudo systemctl restart dockersudo systemctl restart kubeletnetwork plugin is not ready: cni config uninitialized具体影响：每个节点状态永远是 NotReady，无法进行容器调度解决：kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml原问题链接：github issue或者试一试：mkdir /etc/cni/net.dswapkubeadm 添加参数：--ignore-preflight-errors Swapkubelet 参数配置：/etc/sysconfig/kubelet ： --fail-swap-on=false有时 /etc/sysconfig/kubelet 不好使时，可以将参数加在 /var/lib/kubelet/kubeadm-flags.envKUBELET_EXTRA_ARGS=\"--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice --fail-swap-on=false\"输出 pod node status 列表：kubectl get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName --all-namespaceskubeadm CPU 核心需求kubeadm the number of available cpus 1 is less than the required 2它有配置要求： 2 CPUs or more 2 GB or more of RAM per machine (any less will leave little room for your apps) Swap disabled. You MUST disable swap in order for the kubelet to work properly.节点加入后 NotReadykubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml或者mkdir /etc/cni/net.d -psystemctl restart kubelet有时 /etc/sysconfig/kubelet 不好使时，可以将参数加在 /var/lib/kubelet/kubeadm-flags.env问题 issue：github issueFlannel需要在 kubeadm init 时指定 --pod-network-cidr=10.244.0.0/16容器一直处于创建状态查看 systemctl status kubelet ，发现： Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to在 /etc/sysconfig/kubelet 中添加：KUBELET_EXTRA_ARGS=\"--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice\"Issue 链接：github issue阿里云没有公网ip，master 上只看见它的局域网ip：无解创建公网ip：ifconfig eth0:0 IP然后在 /etc/sysconfig/kubelet 中的 KUBELET_EXTRA_ARGS 添加 --node-ip=IP 即可常用命令初始化kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.13.1 --apiserver-advertise-address=IP安装 flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml安装 weave scopekubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"weave scope 开放端口，及 ssh 隧道kubectl port-forward -n weave \"$(kubectl get -n weave pod --selector=weave-scope-component=app -o jsonpath='{.items..metadata.name}')\" 4040ssh -N -L 0.0.0.0:4041:localhost:4040 root@IP -p 22运行 master 节点运行 podkubectl describe node t0 | grep Taints # 查看 taintskubectl taint node t0 node-role.kubernetes.io/master:NoSchedule- # 取消 NoSchedule给节点打标签kubectl label nodes t0 node=node0 # 给节点 t0 打上标签：node=node0kubectl get nodes --show-labels # 查看节点标签清理 cni：ifconfig cni0 downip link delete flannel.1brctl delbr cni0注意 kubeadm reset 后还不够，还需要清理 network interface: cni0 和 flannel.1，否则可能出现无法解析地址端口防火墙方面需要放行，具体参考官方文档：Ports and Protocols Kubernetes" }, { "title": "滴答清单网页版 API 记录", "url": "/posts/DiDaWebAPI/", "categories": "", "tags": "", "date": "2022-02-23 00:00:00 +0800", "snippet": " 官方已更新 API 文档，直接查询官方即可：TickTick Developer因为计算机是我每天学习工作的主力，有时候想在清理滴答清单任务的时候计时，但又发现滴答清单没有计时功能，所以想着后续有时间用滴答清单的 API 写个简单的计时功能，这样可以直接集成不用造 todolist 的轮子了。以下是在研究滴答清单网页版时发现的一些 API。目前官网有意愿放出 API 文档，但是目前只有一个简陋的旧版本文档，所以只能自己动手了。基本说明 非官方，随时可能会失效！业务逻辑cookie 设置 请求头中 cookie 需要包含 t=xxxxx 字段，否则部分请求会出现 500 错误等 xxxxx 为登录API返回的token字段的值API 列表SignOn 地址：https://api.dida365.com/api/v2/user/signon?wc=true&amp;remember=true 类型：POST 关键 Headers： x-device：通过调试工具在网页版滴答清单上获取 body： { \"password\": \"密码\", \"username\": \"邮箱\"} 返回值： { \"token\": \"cookie需要这个东西\", \"userId\": \"xxxxx\", \"username\": \"xxxx\", \"proStartDate\": \"2018-12-13T11:24:50.000+0000\", \"proEndDate\": \"2022-12-13T12:41:14.000+0000\", \"subscribeType\": \"order\", \"needSubscribe\": true, \"inboxId\": \"xxxx\", \"teamUser\": false, \"activeTeamUser\": false, \"freeTrial\": false, \"pro\": true, \"ds\": false} WebSocket 地址：wss://wss.dida365.com/web 说明：接收来自服务器的信息，比如任务更新等 消息： 刚连接时会返回一个 token，用于 PushRegister 中与 websocket 建立消息连接 需要定期向服务端发送心跳包：web版本是每9分钟发送 hello 字符串 服务端发生更新时，会主动发送一个 checkPointID 用于BatchCheck获取变化 PushRegister 地址：https://api.dida365.com/api/v2/push/register 类型：POST body： { \"pushToken\": \"WebSocket返回的token\", \"osType\": 41} BatchCheck 地址：https://api.dida365.com/api/v2/batch/check/{checkPointID} 类型：GET 参数： checkPointID： 0：获取全部 WebSocket 中提供的 ID：获取相较于上一个 checkPoint 的更新内容 返回值： { \"checkPoint\": 1646973127626, // 当前的 checkPointID \"syncTaskBean\": { \t\"update\": [ \t\t{ \t\t\t\"id\": \"任务ID\", \t\t\t\"projectId\": \"projectId\", ...\t\t\t} ],\t\t\"delete\": [],\t\t\"add\": [] },\t\"projectProfiles\": null,\t\"projectGroups\": [ // 项目描述 ... ], \"filters\": null, \"tags\": [], // tag 描述\t\"inboxId\": null // checkPointId 为 0 时值为非 null，表示收件箱 Id，与其他的 Project 区分} " }, { "title": "【转载】Async IO in Python: A Complete Walkthrough", "url": "/posts/Repost-Async-IO-in-Python-A-Complete-Walkthrough/", "categories": "", "tags": "Python, 转载", "date": "2021-09-12 00:57:57 +0800", "snippet": "Async IO is a concurrent programming design that has received dedicated support in Python, evolving rapidly from Python 3.4 through 3.7, and probably beyond.You may be thinking with dread, “Concurrency, parallelism, threading, multiprocessing. That’s a lot to grasp already. Where does async IO fit in?”This tutorial is built to help you answer that question, giving you a firmer grasp of Python’s approach to async IO.Here’s what you’ll cover: Asynchronous IO (async IO): a language-agnostic paradigm (model) that has implementations across a host of programming languages async/await: two new Python keywords that are used to define coroutines asyncio: the Python package that provides a foundation and API for running and managing coroutinesCoroutines (specialized generator functions) are the heart of async IO in Python, and we’ll dive into them later on. Note: In this article, I use the term async IO to denote the language-agnostic design of asynchronous IO, while asyncio refers to the Python package.Before you get started, you’ll need to make sure you’re set up to use asyncio and other libraries found in this tutorial.Setting Up Your EnvironmentYou’ll need Python 3.7 or above to follow this article in its entirety, as well as the aiohttp and aiofiles packages:$ python3.7 -m venv ./py37async$ source ./py37async/bin/activate # Windows: .\\py37async\\Scripts\\activate.bat$ pip install --upgrade pip aiohttp aiofiles # Optional: aiodnsFor help with installing Python 3.7 and setting up a virtual environment, check out Python 3 Installation &amp; Setup Guide or Virtual Environments Primer.With that, let’s jump in.The 10,000-Foot View of Async IOAsync IO is a bit lesser known than its tried-and-true cousins, multiprocessing and threading. This section will give you a fuller picture of what async IO is and how it fits into its surrounding landscape.Where Does Async IO Fit In?Concurrency and parallelism are expansive subjects that are not easy to wade into. While this article focuses on async IO and its implementation in Python, it’s worth taking a minute to compare async IO to its counterparts in order to have context about how async IO fits into the larger, sometimes dizzying puzzle. Parallelism consists of performing multiple operations at the same time. Multiprocessing is a means to effect parallelism, and it entails spreading tasks over a computer’s central processing units (CPUs, or cores). Multiprocessing is well-suited for CPU-bound tasks: tightly bound for loops and mathematical computations usually fall into this category. Concurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.) Threading is a concurrent execution model whereby multiple threads take turns executing tasks. One process can contain multiple threads. Python has a complicated relationship with threading thanks to its GIL, but that’s beyond the scope of this article. What’s important to know about threading is that it’s better for IO-bound tasks. While a CPU-bound task is characterized by the computer’s cores continually working hard from start to finish, an IO-bound job is dominated by a lot of waiting on input/output to complete.To recap the above, concurrency encompasses both multiprocessing (ideal for CPU-bound tasks) and threading (suited for IO-bound tasks). Multiprocessing is a form of parallelism, with parallelism being a specific type (subset) of concurrency. The Python standard library has offered longstanding support for both of these through its multiprocessing, threading, and concurrent.futures packages.Now it’s time to bring a new member to the mix. Over the last few years, a separate design has been more comprehensively built into CPython: asynchronous IO, enabled through the standard library’s asyncio package and the new async and await language keywords. To be clear, async IO is not a newly invented concept, and it has existed or is being built into other languages and runtime environments, such as Go, C#, or Scala.The asyncio package is billed by the Python documentation as a library to write concurrent code. However, async IO is not threading, nor is it multiprocessing. It is not built on top of either of these.In fact, **async IO is a single-threaded, single-process design**: it uses cooperative multitasking, a term that you’ll flesh out by the end of this tutorial. It has been said in other words that async IO gives a feeling of concurrency despite using a single thread in a single process. Coroutines (a central feature of async IO) can be scheduled concurrently, but they are not inherently concurrent.To reiterate, async IO is a style of concurrent programming, but it is not parallelism. It’s more closely aligned with threading than with multiprocessing but is very much distinct from both of these and is a standalone member in concurrency’s bag of tricks.That leaves one more term. What does it mean for something to be asynchronous? This isn’t a rigorous definition, but for our purposes here, I can think of two properties: Asynchronous routines are able to “pause” while waiting on their ultimate result and let other routines run in the meantime. Asynchronous code, through the mechanism above, facilitates concurrent execution. To put it differently, asynchronous code gives the look and feel of concurrency.Here’s a diagram to put it all together. The white terms represent concepts, and the green terms represent ways in which they are implemented or effected:I’ll stop there on the comparisons between concurrent programming models. This tutorial is focused on the subcomponent that is async IO, how to use it, and the APIs that have sprung up around it. For a thorough exploration of threading versus multiprocessing versus async IO, pause here and check out Jim Anderson’s overview of concurrency in Python. Jim is way funnier than me and has sat in more meetings than me, to boot.Async IO ExplainedAsync IO may at first seem counterintuitive and paradoxical. How does something that facilitates concurrent code use a single thread and a single CPU core? I’ve never been very good at conjuring up examples, so I’d like to paraphrase one from Miguel Grinberg’s 2017 PyCon talk, which explains everything quite beautifully: Chess master Judit Polgár hosts a chess exhibition in which she plays multiple amateur players. She has two ways of conducting the exhibition: synchronously and asynchronously. Assumptions: 24 opponents Judit makes each chess move in 5 seconds Opponents each take 55 seconds to make a move Games average 30 pair-moves (60 moves total) Synchronous version: Judit plays one game at a time, never two at the same time, until the game is complete. Each game takes (55 + 5) * 30 == 1800 seconds, or 30 minutes. The entire exhibition takes 24 * 30 == 720 minutes, or 12 hours. Asynchronous version: Judit moves from table to table, making one move at each table. She leaves the table and lets the opponent make their next move during the wait time. One move on all 24 games takes Judit 24 * 5 == 120 seconds, or 2 minutes. The entire exhibition is now cut down to 120 * 30 == 3600 seconds, or just 1 hour. (Source)There is only one Judit Polgár, who has only two hands and makes only one move at a time by herself. But playing asynchronously cuts the exhibition time down from 12 hours to one. So, cooperative multitasking is a fancy way of saying that a program’s event loop (more on that later) communicates with multiple tasks to let each take turns running at the optimal time.Async IO takes long waiting periods in which functions would otherwise be blocking and allows other functions to run during that downtime. (A function that blocks effectively forbids others from running from the time that it starts until the time that it returns.)Async IO Is Not EasyI’ve heard it said, “Use async IO when you can; use threading when you must.” The truth is that building durable multithreaded code can be hard and error-prone. Async IO avoids some of the potential speedbumps that you might otherwise encounter with a threaded design.But that’s not to say that async IO in Python is easy. Be warned: when you venture a bit below the surface level, async programming can be difficult too! Python’s async model is built around concepts such as callbacks, events, transports, protocols, and futures—just the terminology can be intimidating. The fact that its API has been changing continually makes it no easier.Luckily, asyncio has matured to a point where most of its features are no longer provisional, while its documentation has received a huge overhaul and some quality resources on the subject are starting to emerge as well.The asyncio Package and async/awaitNow that you have some background on async IO as a design, let’s explore Python’s implementation. Python’s asyncio package (introduced in Python 3.4) and its two keywords, async and await, serve different purposes but come together to help you declare, build, execute, and manage asynchronous code.The async/await Syntax and Native Coroutines A Word of Caution: Be careful what you read out there on the Internet. Python’s async IO API has evolved rapidly from Python 3.4 to Python 3.7. Some old patterns are no longer used, and some things that were at first disallowed are now allowed through new introductions.At the heart of async IO are coroutines. A coroutine is a specialized version of a Python generator function. Let’s start with a baseline definition and then build off of it as you progress here: a coroutine is a function that can suspend its execution before reaching return, and it can indirectly pass control to another coroutine for some time.Later, you’ll dive a lot deeper into how exactly the traditional generator is repurposed into a coroutine. For now, the easiest way to pick up how coroutines work is to start making some.Let’s take the immersive approach and write some async IO code. This short program is the Hello World of async IO but goes a long way towards illustrating its core functionality:#!/usr/bin/env python3# countasync.pyimport asyncioasync def count(): print(\"One\") await asyncio.sleep(1) print(\"Two\")async def main(): await asyncio.gather(count(), count(), count())if __name__ == \"__main__\": import time s = time.perf_counter() asyncio.run(main()) elapsed = time.perf_counter() - s print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")When you execute this file, take note of what looks different than if you were to define the functions with just def and time.sleep():$ python3 countasync.pyOneOneOneTwoTwoTwocountasync.py executed in 1.01 seconds.The order of this output is the heart of async IO. Talking to each of the calls to count() is a single event loop, or coordinator. When each task reaches await asyncio.sleep(1), the function yells up to the event loop and gives control back to it, saying, “I’m going to be sleeping for 1 second. Go ahead and let something else meaningful be done in the meantime.”Contrast this to the synchronous version:#!/usr/bin/env python3# countsync.pyimport timedef count(): print(\"One\") time.sleep(1) print(\"Two\")def main(): for _ in range(3): count()if __name__ == \"__main__\": s = time.perf_counter() main() elapsed = time.perf_counter() - s print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")When executed, there is a slight but critical change in order and execution time:$ python3 countsync.pyOneTwoOneTwoOneTwocountsync.py executed in 3.01 seconds.While using time.sleep() and asyncio.sleep() may seem banal, they are used as stand-ins for any time-intensive processes that involve wait time. (The most mundane thing you can wait on is a sleep() call that does basically nothing.) That is, time.sleep() can represent any time-consuming blocking function call, while asyncio.sleep() is used to stand in for a non-blocking call (but one that also takes some time to complete).As you’ll see in the next section, the benefit of awaiting something, including asyncio.sleep(), is that the surrounding function can temporarily cede control to another function that’s more readily able to do something immediately. In contrast, `time.sleep()` or any other blocking call is incompatible with asynchronous Python code, because it will stop everything in its tracks for the duration of the sleep time.The Rules of Async IOAt this point, a more formal definition of async, await, and the coroutine functions that they create are in order. This section is a little dense, but getting a hold of async/await is instrumental, so come back to this if you need to: The syntax async def introduces either a native coroutine or an asynchronous generator. The expressions async with and async for are also valid, and you’ll see them later on. The keyword await passes function control back to the event loop. (It suspends the execution of the surrounding coroutine.) If Python encounters an await f() expression in the scope of g(), this is how await tells the event loop, “Suspend execution of g() until whatever I’m waiting on—the result of f()—is returned. In the meantime, go let something else run.”In code, that second bullet point looks roughly like this:async def g(): # Pause here and come back to g() when f() is ready r = await f() return rThere’s also a strict set of rules around when and how you can and cannot use async/await. These can be handy whether you are still picking up the syntax or already have exposure to using async/await: A function that you introduce with async def is a coroutine. It may use await, return, or yield, but all of these are optional. Declaring async def noop(): pass is valid: Using await and/or return creates a coroutine function. To call a coroutine function, you must await it to get its results. It is less common (and only recently legal in Python) to use yield in an async def block. This creates an asynchronous generator, which you iterate over with async for. Forget about async generators for the time being and focus on getting down the syntax for coroutine functions, which use await and/or return. Anything defined with async def may not use yield from, which will raise a SyntaxError. Just like it’s a SyntaxError to use yield outside of a def function, it is a SyntaxError to use await outside of an async def coroutine. You can only use await in the body of coroutines.Here are some terse examples meant to summarize the above few rules:async def f(x): y = await z(x) # OK - `await` and `return` allowed in coroutines return yasync def g(x): yield x # OK - this is an async generatorasync def m(x): yield from gen(x) # No - SyntaxErrordef m(x): y = await z(x) # Still no - SyntaxError (no `async def` here) return yFinally, when you use await f(), it’s required that f() be an object that is awaitable. Well, that’s not very helpful, is it? For now, just know that an awaitable object is either (1) another coroutine or (2) an object defining an .__await__() dunder method that returns an iterator. If you’re writing a program, for the large majority of purposes, you should only need to worry about case #1.That brings us to one more technical distinction that you may see pop up: an older way of marking a function as a coroutine is to decorate a normal def function with @asyncio.coroutine. The result is a generator-based coroutine. This construction has been outdated since the async/await syntax was put in place in Python 3.5.These two coroutines are essentially equivalent (both are awaitable), but the first is generator-based, while the second is a native coroutine:import asyncio@asyncio.coroutinedef py34_coro(): \"\"\"Generator-based coroutine, older syntax\"\"\" yield from stuff()async def py35_coro(): \"\"\"Native coroutine, modern syntax\"\"\" await stuff()If you’re writing any code yourself, prefer native coroutines for the sake of being explicit rather than implicit. Generator-based coroutines will be removed in Python 3.10.Towards the latter half of this tutorial, we’ll touch on generator-based coroutines for explanation’s sake only. The reason that async/await were introduced is to make coroutines a standalone feature of Python that can be easily differentiated from a normal generator function, thus reducing ambiguity.Don’t get bogged down in generator-based coroutines, which have been deliberately outdated by async/await. They have their own small set of rules (for instance, await cannot be used in a generator-based coroutine) that are largely irrelevant if you stick to the async/await syntax.Without further ado, let’s take on a few more involved examples.Here’s one example of how async IO cuts down on wait time: given a coroutine makerandom() that keeps producing random integers in the range [0, 10], until one of them exceeds a threshold, you want to let multiple calls of this coroutine not need to wait for each other to complete in succession. You can largely follow the patterns from the two scripts above, with slight changes:#!/usr/bin/env python3# rand.pyimport asyncioimport random# ANSI colorsc = ( \"\\033[0m\", # End of color \"\\033[36m\", # Cyan \"\\033[91m\", # Red \"\\033[35m\", # Magenta)async def makerandom(idx: int, threshold: int = 6) -&gt; int: print(c[idx + 1] + f\"Initiated makerandom({idx}).\") i = random.randint(0, 10) while i &lt;= threshold: print(c[idx + 1] + f\"makerandom({idx}) == {i} too low; retrying.\") await asyncio.sleep(idx + 1) i = random.randint(0, 10) print(c[idx + 1] + f\"---&gt; Finished: makerandom({idx}) == {i}\" + c[0]) return iasync def main(): res = await asyncio.gather(*(makerandom(i, 10 - i - 1) for i in range(3))) return resif __name__ == \"__main__\": random.seed(444) r1, r2, r3 = asyncio.run(main()) print() print(f\"r1: {r1}, r2: {r2}, r3: {r3}\")The colorized output says a lot more than I can and gives you a sense for how this script is carried out:This program uses one main coroutine, makerandom(), and runs it concurrently across 3 different inputs. Most programs will contain small, modular coroutines and one wrapper function that serves to chain each of the smaller coroutines together. main() is then used to gather tasks (futures) by mapping the central coroutine across some iterable or pool.In this miniature example, the pool is range(3). In a fuller example presented later, it is a set of URLs that need to be requested, parsed, and processed concurrently, and main() encapsulates that entire routine for each URL.While “making random integers” (which is CPU-bound more than anything) is maybe not the greatest choice as a candidate for asyncio, it’s the presence of asyncio.sleep() in the example that is designed to mimic an IO-bound process where there is uncertain wait time involved. For example, the asyncio.sleep() call might represent sending and receiving not-so-random integers between two clients in a message application.Async IO Design PatternsAsync IO comes with its own set of possible script designs, which you’ll get introduced to in this section.Chaining CoroutinesA key feature of coroutines is that they can be chained together. (Remember, a coroutine object is awaitable, so another coroutine can await it.) This allows you to break programs into smaller, manageable, recyclable coroutines:#!/usr/bin/env python3# chained.pyimport asyncioimport randomimport timeasync def part1(n: int) -&gt; str: i = random.randint(0, 10) print(f\"part1({n}) sleeping for {i} seconds.\") await asyncio.sleep(i) result = f\"result{n}-1\" print(f\"Returning part1({n}) == {result}.\") return resultasync def part2(n: int, arg: str) -&gt; str: i = random.randint(0, 10) print(f\"part2{n, arg} sleeping for {i} seconds.\") await asyncio.sleep(i) result = f\"result{n}-2 derived from {arg}\" print(f\"Returning part2{n, arg} == {result}.\") return resultasync def chain(n: int) -&gt; None: start = time.perf_counter() p1 = await part1(n) p2 = await part2(n, p1) end = time.perf_counter() - start print(f\"--&gt;Chained result{n} =&gt; {p2} (took {end:0.2f} seconds).\")async def main(*args): await asyncio.gather(*(chain(n) for n in args))if __name__ == \"__main__\": import sys random.seed(444) args = [1, 2, 3] if len(sys.argv) == 1 else map(int, sys.argv[1:]) start = time.perf_counter() asyncio.run(main(*args)) end = time.perf_counter() - start print(f\"Program finished in {end:0.2f} seconds.\")Pay careful attention to the output, where part1() sleeps for a variable amount of time, and part2() begins working with the results as they become available:$ python3 chained.py 9 6 3part1(9) sleeping for 4 seconds.part1(6) sleeping for 4 seconds.part1(3) sleeping for 0 seconds.Returning part1(3) == result3-1.part2(3, 'result3-1') sleeping for 4 seconds.Returning part1(9) == result9-1.part2(9, 'result9-1') sleeping for 7 seconds.Returning part1(6) == result6-1.part2(6, 'result6-1') sleeping for 4 seconds.Returning part2(3, 'result3-1') == result3-2 derived from result3-1.--&gt;Chained result3 =&gt; result3-2 derived from result3-1 (took 4.00 seconds).Returning part2(6, 'result6-1') == result6-2 derived from result6-1.--&gt;Chained result6 =&gt; result6-2 derived from result6-1 (took 8.01 seconds).Returning part2(9, 'result9-1') == result9-2 derived from result9-1.--&gt;Chained result9 =&gt; result9-2 derived from result9-1 (took 11.01 seconds).Program finished in 11.01 seconds.In this setup, the runtime of main() will be equal to the maximum runtime of the tasks that it gathers together and schedules.Using a QueueThe asyncio package provides queue classes that are designed to be similar to classes of the queue module. In our examples so far, we haven’t really had a need for a queue structure. In chained.py, each task (future) is composed of a set of coroutines that explicitly await each other and pass through a single input per chain.There is an alternative structure that can also work with async IO: a number of producers, which are not associated with each other, add items to a queue. Each producer may add multiple items to the queue at staggered, random, unannounced times. A group of consumers pull items from the queue as they show up, greedily and without waiting for any other signal.In this design, there is no chaining of any individual consumer to a producer. The consumers don’t know the number of producers, or even the cumulative number of items that will be added to the queue, in advance.It takes an individual producer or consumer a variable amount of time to put and extract items from the queue, respectively. The queue serves as a throughput that can communicate with the producers and consumers without them talking to each other directly. Note: While queues are often used in threaded programs because of the thread-safety of queue.Queue(), you shouldn’t need to concern yourself with thread safety when it comes to async IO. (The exception is when you’re combining the two, but that isn’t done in this tutorial.) One use-case for queues (as is the case here) is for the queue to act as a transmitter for producers and consumers that aren’t otherwise directly chained or associated with each other.The synchronous version of this program would look pretty dismal: a group of blocking producers serially add items to the queue, one producer at a time. Only after all producers are done can the queue be processed, by one consumer at a time processing item-by-item. There is a ton of latency in this design. Items may sit idly in the queue rather than be picked up and processed immediately.An asynchronous version, asyncq.py, is below. The challenging part of this workflow is that there needs to be a signal to the consumers that production is done. Otherwise, await q.get() will hang indefinitely, because the queue will have been fully processed, but consumers won’t have any idea that production is complete.(Big thanks for some help from a StackOverflow user for helping to straighten out main(): the key is to await q.join(), which blocks until all items in the queue have been received and processed, and then to cancel the consumer tasks, which would otherwise hang up and wait endlessly for additional queue items to appear.)Here is the full script:#!/usr/bin/env python3# asyncq.pyimport asyncioimport itertools as itimport osimport randomimport timeasync def makeitem(size: int = 5) -&gt; str: return os.urandom(size).hex()async def randsleep(caller=None) -&gt; None: i = random.randint(0, 10) if caller: print(f\"{caller} sleeping for {i} seconds.\") await asyncio.sleep(i)async def produce(name: int, q: asyncio.Queue) -&gt; None: n = random.randint(0, 10) for _ in it.repeat(None, n): # Synchronous loop for each single producer await randsleep(caller=f\"Producer {name}\") i = await makeitem() t = time.perf_counter() await q.put((i, t)) print(f\"Producer {name} added &lt;{i}&gt; to queue.\")async def consume(name: int, q: asyncio.Queue) -&gt; None: while True: await randsleep(caller=f\"Consumer {name}\") i, t = await q.get() now = time.perf_counter() print(f\"Consumer {name} got element &lt;{i}&gt;\" f\" in {now-t:0.5f} seconds.\") q.task_done()async def main(nprod: int, ncon: int): q = asyncio.Queue() producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)] consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)] await asyncio.gather(*producers) await q.join() # Implicitly awaits consumers, too for c in consumers: c.cancel()if __name__ == \"__main__\": import argparse random.seed(444) parser = argparse.ArgumentParser() parser.add_argument(\"-p\", \"--nprod\", type=int, default=5) parser.add_argument(\"-c\", \"--ncon\", type=int, default=10) ns = parser.parse_args() start = time.perf_counter() asyncio.run(main(**ns.__dict__)) elapsed = time.perf_counter() - start print(f\"Program completed in {elapsed:0.5f} seconds.\")The first few coroutines are helper functions that return a random string, a fractional-second performance counter, and a random integer. A producer puts anywhere from 1 to 5 items into the queue. Each item is a tuple of (i, t) where i is a random string and t is the time at which the producer attempts to put the tuple into the queue.When a consumer pulls an item out, it simply calculates the elapsed time that the item sat in the queue using the timestamp that the item was put in with.Keep in mind that asyncio.sleep() is used to mimic some other, more complex coroutine that would eat up time and block all other execution if it were a regular blocking function.Here is a test run with two producers and five consumers:$ python3 asyncq.py -p 2 -c 5Producer 0 sleeping for 3 seconds.Producer 1 sleeping for 3 seconds.Consumer 0 sleeping for 4 seconds.Consumer 1 sleeping for 3 seconds.Consumer 2 sleeping for 3 seconds.Consumer 3 sleeping for 5 seconds.Consumer 4 sleeping for 4 seconds.Producer 0 added &lt;377b1e8f82&gt; to queue.Producer 0 sleeping for 5 seconds.Producer 1 added &lt;413b8802f8&gt; to queue.Consumer 1 got element &lt;377b1e8f82&gt; in 0.00013 seconds.Consumer 1 sleeping for 3 seconds.Consumer 2 got element &lt;413b8802f8&gt; in 0.00009 seconds.Consumer 2 sleeping for 4 seconds.Producer 0 added &lt;06c055b3ab&gt; to queue.Producer 0 sleeping for 1 seconds.Consumer 0 got element &lt;06c055b3ab&gt; in 0.00021 seconds.Consumer 0 sleeping for 4 seconds.Producer 0 added &lt;17a8613276&gt; to queue.Consumer 4 got element &lt;17a8613276&gt; in 0.00022 seconds.Consumer 4 sleeping for 5 seconds.Program completed in 9.00954 seconds.In this case, the items process in fractions of a second. A delay can be due to two reasons: Standard, largely unavoidable overhead Situations where all consumers are sleeping when an item appears in the queueWith regards to the second reason, luckily, it is perfectly normal to scale to hundreds or thousands of consumers. You should have no problem with python3 asyncq.py -p 5 -c 100. The point here is that, theoretically, you could have different users on different systems controlling the management of producers and consumers, with the queue serving as the central throughput.So far, you’ve been thrown right into the fire and seen three related examples of asyncio calling coroutines defined with async and await. If you’re not completely following or just want to get deeper into the mechanics of how modern coroutines came to be in Python, you’ll start from square one with the next section.Async IO’s Roots in GeneratorsEarlier, you saw an example of the old-style generator-based coroutines, which have been outdated by more explicit native coroutines. The example is worth re-showing with a small tweak:import asyncio@asyncio.coroutinedef py34_coro(): \"\"\"Generator-based coroutine\"\"\" # No need to build these yourself, but be aware of what they are s = yield from stuff() return sasync def py35_coro(): \"\"\"Native coroutine, modern syntax\"\"\" s = await stuff() return sasync def stuff(): return 0x10, 0x20, 0x30As an experiment, what happens if you call py34_coro() or py35_coro() on its own, without await, or without any calls to asyncio.run() or other asyncio “porcelain” functions? Calling a coroutine in isolation returns a coroutine object:&gt;&gt;&gt; py35_coro()&lt;coroutine object py35_coro at 0x10126dcc8&gt;This isn’t very interesting on its surface. The result of calling a coroutine on its own is an awaitable coroutine object.Time for a quiz: what other feature of Python looks like this? (What feature of Python doesn’t actually “do much” when it’s called on its own?)Hopefully you’re thinking of generators as an answer to this question, because coroutines are enhanced generators under the hood. The behavior is similar in this regard:&gt;&gt;&gt; def gen():... yield 0x10, 0x20, 0x30...&gt;&gt;&gt; g = gen()&gt;&gt;&gt; g # Nothing much happens - need to iterate with `.__next__()`&lt;generator object gen at 0x1012705e8&gt;&gt;&gt;&gt; next(g)(16, 32, 48)Generator functions are, as it so happens, the foundation of async IO (regardless of whether you declare coroutines with async def rather than the older @asyncio.coroutine wrapper). Technically, await is more closely analogous to yield from than it is to yield. (But remember that yield from x() is just syntactic sugar to replace for i in x(): yield i.)One critical feature of generators as it pertains to async IO is that they can effectively be stopped and restarted at will. For example, you can break out of iterating over a generator object and then resume iteration on the remaining values later. When a generator function reaches yield, it yields that value, but then it sits idle until it is told to yield its subsequent value.This can be fleshed out through an example:&gt;&gt;&gt; from itertools import cycle&gt;&gt;&gt; def endless():... \"\"\"Yields 9, 8, 7, 6, 9, 8, 7, 6, ... forever\"\"\"... yield from cycle((9, 8, 7, 6))&gt;&gt;&gt; e = endless()&gt;&gt;&gt; total = 0&gt;&gt;&gt; for i in e:... if total &lt; 30:... print(i, end=\" \")... total += i... else:... print()... # Pause execution. We can resume later.... break9 8 7 6 9 8 7 6 9 8 7 6 9 8&gt;&gt;&gt; # Resume&gt;&gt;&gt; next(e), next(e), next(e)(6, 9, 8)The await keyword behaves similarly, marking a break point at which the coroutine suspends itself and lets other coroutines work. “Suspended,” in this case, means a coroutine that has temporarily ceded control but not totally exited or finished. Keep in mind that yield, and by extension yield from and await, mark a break point in a generator’s execution.This is the fundamental difference between functions and generators. A function is all-or-nothing. Once it starts, it won’t stop until it hits a return, then pushes that value to the caller (the function that calls it). A generator, on the other hand, pauses each time it hits a yield and goes no further. Not only can it push this value to calling stack, but it can keep a hold of its local variables when you resume it by calling next() on it.There’s a second and lesser-known feature of generators that also matters. You can send a value into a generator as well through its .send() method. This allows generators (and coroutines) to call (await) each other without blocking. I won’t get any further into the nuts and bolts of this feature, because it matters mainly for the implementation of coroutines behind the scenes, but you shouldn’t ever really need to use it directly yourself.If you’re interested in exploring more, you can start at PEP 342, where coroutines were formally introduced. Brett Cannon’s How the Heck Does Async-Await Work in Python is also a good read, as is the PYMOTW writeup on asyncio. Lastly, there’s David Beazley’s Curious Course on Coroutines and Concurrency, which dives deep into the mechanism by which coroutines run.Let’s try to condense all of the above articles into a few sentences: there is a particularly unconventional mechanism by which these coroutines actually get run. Their result is an attribute of the exception object that gets thrown when their .send() method is called. There’s some more wonky detail to all of this, but it probably won’t help you use this part of the language in practice, so let’s move on for now.To tie things together, here are some key points on the topic of coroutines as generators: Coroutines are repurposed generators that take advantage of the peculiarities of generator methods. Old generator-based coroutines use yield from to wait for a coroutine result. Modern Python syntax in native coroutines simply replaces yield from with await as the means of waiting on a coroutine result. The await is analogous to yield from, and it often helps to think of it as such. The use of await is a signal that marks a break point. It lets a coroutine temporarily suspend execution and permits the program to come back to it later.Other Features: async for and Async Generators + ComprehensionsAlong with plain async/await, Python also enables async for to iterate over an asynchronous iterator. The purpose of an asynchronous iterator is for it to be able to call asynchronous code at each stage when it is iterated over.A natural extension of this concept is an asynchronous generator. Recall that you can use await, return, or yield in a native coroutine. Using yield within a coroutine became possible in Python 3.6 (via PEP 525), which introduced asynchronous generators with the purpose of allowing await and yield to be used in the same coroutine function body:&gt;&gt;&gt; async def mygen(u: int = 10):... \"\"\"Yield powers of 2.\"\"\"... i = 0... while i &lt; u:... yield 2 ** i... i += 1... await asyncio.sleep(0.1)Last but not least, Python enables asynchronous comprehension with async for. Like its synchronous cousin, this is largely syntactic sugar:&gt;&gt;&gt; async def main():... # This does *not* introduce concurrent execution... # It is meant to show syntax only... g = [i async for i in mygen()]... f = [j async for j in mygen() if not (j // 3 % 5)]... return g, f...&gt;&gt;&gt; g, f = asyncio.run(main())&gt;&gt;&gt; g[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]&gt;&gt;&gt; f[1, 2, 16, 32, 256, 512]This is a crucial distinction: neither asynchronous generators nor comprehensions make the iteration concurrent. All that they do is provide the look-and-feel of their synchronous counterparts, but with the ability for the loop in question to give up control to the event loop for some other coroutine to run.In other words, asynchronous iterators and asynchronous generators are not designed to concurrently map some function over a sequence or iterator. They’re merely designed to let the enclosing coroutine allow other tasks to take their turn. The async for and async with statements are only needed to the extent that using plain for or with would “break” the nature of await in the coroutine. This distinction between asynchronicity and concurrency is a key one to grasp.The Event Loop and asyncio.run()You can think of an event loop as something like a while True loop that monitors coroutines, taking feedback on what’s idle, and looking around for things that can be executed in the meantime. It is able to wake up an idle coroutine when whatever that coroutine is waiting on becomes available.Thus far, the entire management of the event loop has been implicitly handled by one function call:asyncio.run(main()) # Python 3.7+asyncio.run(), introduced in Python 3.7, is responsible for getting the event loop, running tasks until they are marked as complete, and then closing the event loop.There’s a more long-winded way of managing the asyncio event loop, with get_event_loop(). The typical pattern looks like this:loop = asyncio.get_event_loop()try: loop.run_until_complete(main())finally: loop.close()You’ll probably see loop.get_event_loop() floating around in older examples, but unless you have a specific need to fine-tune control over the event loop management, asyncio.run() should be sufficient for most programs.If you do need to interact with the event loop within a Python program, loop is a good-old-fashioned Python object that supports introspection with loop.is_running() and loop.is_closed(). You can manipulate it if you need to get more fine-tuned control, such as in scheduling a callback by passing the loop as an argument.What is more crucial is understanding a bit beneath the surface about the mechanics of the event loop. Here are a few points worth stressing about the event loop.#1: Coroutines don’t do much on their own until they are tied to the event loop.You saw this point before in the explanation on generators, but it’s worth restating. If you have a main coroutine that awaits others, simply calling it in isolation has little effect:&gt;&gt;&gt; import asyncio&gt;&gt;&gt; async def main():... print(\"Hello ...\")... await asyncio.sleep(1)... print(\"World!\")&gt;&gt;&gt; routine = main()&gt;&gt;&gt; routine&lt;coroutine object main at 0x1027a6150&gt;Remember to use asyncio.run() to actually force execution by scheduling the main() coroutine (future object) for execution on the event loop:&gt;&gt;&gt; asyncio.run(routine)Hello ...World!(Other coroutines can be executed with await. It is typical to wrap just main() in asyncio.run(), and chained coroutines with await will be called from there.)#2: By default, an async IO event loop runs in a single thread and on a single CPU core. Usually, running one single-threaded event loop in one CPU core is more than sufficient. It is also possible to run event loops across multiple cores. Check out this talk by John Reese for more, and be warned that your laptop may spontaneously combust.#3. Event loops are pluggable. That is, you could, if you really wanted, write your own event loop implementation and have it run tasks just the same. This is wonderfully demonstrated in the uvloop package, which is an implementation of the event loop in Cython.That is what is meant by the term “pluggable event loop”: you can use any working implementation of an event loop, unrelated to the structure of the coroutines themselves. The asyncio package itself ships with two different event loop implementations, with the default being based on the selectors module. (The second implementation is built for Windows only.)A Full Program: Asynchronous RequestsYou’ve made it this far, and now it’s time for the fun and painless part. In this section, you’ll build a web-scraping URL collector, areq.py, using aiohttp, a blazingly fast async HTTP client/server framework. (We just need the client part.) Such a tool could be used to map connections between a cluster of sites, with the links forming a directed graph. Note: You may be wondering why Python’s requests package isn’t compatible with async IO. requests is built on top of urllib3, which in turn uses Python’s http and socket modules. By default, socket operations are blocking. This means that Python won’t like await requests.get(url) because .get() is not awaitable. In contrast, almost everything in aiohttp is an awaitable coroutine, such as session.request() and response.text(). It’s a great package otherwise, but you’re doing yourself a disservice by using requests in asynchronous code.The high-level program structure will look like this: Read a sequence of URLs from a local file, urls.txt. Send GET requests for the URLs and decode the resulting content. If this fails, stop there for a URL. Search for the URLs within href tags in the HTML of the responses. Write the results to foundurls.txt. Do all of the above as asynchronously and concurrently as possible. (Use aiohttp for the requests, and aiofiles for the file-appends. These are two primary examples of IO that are well-suited for the async IO model.)Here are the contents of urls.txt. It’s not huge, and contains mostly highly trafficked sites:$ cat urls.txthttps://regex101.com/https://docs.python.org/3/this-url-will-404.htmlhttps://www.nytimes.com/guides/https://www.mediamatters.org/https://1.1.1.1/https://www.politico.com/tipsheets/morning-moneyhttps://www.bloomberg.com/markets/economicshttps://www.ietf.org/rfc/rfc2616.txtThe second URL in the list should return a 404 response, which you’ll need to handle gracefully. If you’re running an expanded version of this program, you’ll probably need to deal with much hairier problems than this, such a server disconnections and endless redirects.The requests themselves should be made using a single session, to take advantage of reusage of the session’s internal connection pool.Let’s take a look at the full program. We’ll walk through things step-by-step after:#!/usr/bin/env python3# areq.py\"\"\"Asynchronously get links embedded in multiple pages' HMTL.\"\"\"import asyncioimport loggingimport reimport sysfrom typing import IOimport urllib.errorimport urllib.parseimport aiofilesimport aiohttpfrom aiohttp import ClientSessionlogging.basicConfig( format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\", level=logging.DEBUG, datefmt=\"%H:%M:%S\", stream=sys.stderr,)logger = logging.getLogger(\"areq\")logging.getLogger(\"chardet.charsetprober\").disabled = TrueHREF_RE = re.compile(r'href=\"(.*?)\"')async def fetch_html(url: str, session: ClientSession, **kwargs) -&gt; str: \"\"\"GET request wrapper to fetch page HTML. kwargs are passed to `session.request()`. \"\"\" resp = await session.request(method=\"GET\", url=url, **kwargs) resp.raise_for_status() logger.info(\"Got response [%s] for URL: %s\", resp.status, url) html = await resp.text() return htmlasync def parse(url: str, session: ClientSession, **kwargs) -&gt; set: \"\"\"Find HREFs in the HTML of `url`.\"\"\" found = set() try: html = await fetch_html(url=url, session=session, **kwargs) except ( aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, ) as e: logger.error( \"aiohttp exception for %s [%s]: %s\", url, getattr(e, \"status\", None), getattr(e, \"message\", None), ) return found except Exception as e: logger.exception( \"Non-aiohttp exception occured: %s\", getattr(e, \"__dict__\", {}) ) return found else: for link in HREF_RE.findall(html): try: abslink = urllib.parse.urljoin(url, link) except (urllib.error.URLError, ValueError): logger.exception(\"Error parsing URL: %s\", link) pass else: found.add(abslink) logger.info(\"Found %d links for %s\", len(found), url) return foundasync def write_one(file: IO, url: str, **kwargs) -&gt; None: \"\"\"Write the found HREFs from `url` to `file`.\"\"\" res = await parse(url=url, **kwargs) if not res: return None async with aiofiles.open(file, \"a\") as f: for p in res: await f.write(f\"{url}\\t{p}\\n\") logger.info(\"Wrote results for source URL: %s\", url)async def bulk_crawl_and_write(file: IO, urls: set, **kwargs) -&gt; None: \"\"\"Crawl &amp; write concurrently to `file` for multiple `urls`.\"\"\" async with ClientSession() as session: tasks = [] for url in urls: tasks.append( write_one(file=file, url=url, session=session, **kwargs) ) await asyncio.gather(*tasks)if __name__ == \"__main__\": import pathlib import sys assert sys.version_info &gt;= (3, 7), \"Script requires Python 3.7+.\" here = pathlib.Path(__file__).parent with open(here.joinpath(\"urls.txt\")) as infile: urls = set(map(str.strip, infile)) outpath = here.joinpath(\"foundurls.txt\") with open(outpath, \"w\") as outfile: outfile.write(\"source_url\\tparsed_url\\n\") asyncio.run(bulk_crawl_and_write(file=outpath, urls=urls))This script is longer than our initial toy programs, so let’s break it down.The constant HREF_RE is a regular expression to extract what we’re ultimately searching for, href tags within HTML:&gt;&gt;&gt; HREF_RE.search('Go to &lt;a href=\"https://realpython.com/\"&gt;Real Python&lt;/a&gt;')&lt;re.Match object; span=(15, 45), match='href=\"https://realpython.com/\"'&gt;The coroutine fetch_html() is a wrapper around a GET request to make the request and decode the resulting page HTML. It makes the request, awaits the response, and raises right away in the case of a non-200 status:resp = await session.request(method=\"GET\", url=url, **kwargs)resp.raise_for_status()If the status is okay, fetch_html() returns the page HTML (a str). Notably, there is no exception handling done in this function. The logic is to propagate that exception to the caller and let it be handled there:html = await resp.text()We await session.request() and resp.text() because they’re awaitable coroutines. The request/response cycle would otherwise be the long-tailed, time-hogging portion of the application, but with async IO, fetch_html() lets the event loop work on other readily available jobs such as parsing and writing URLs that have already been fetched.Next in the chain of coroutines comes parse(), which waits on fetch_html() for a given URL, and then extracts all of the href tags from that page’s HTML, making sure that each is valid and formatting it as an absolute path.Admittedly, the second portion of parse() is blocking, but it consists of a quick regex match and ensuring that the links discovered are made into absolute paths.In this specific case, this synchronous code should be quick and inconspicuous. But just remember that any line within a given coroutine will block other coroutines unless that line uses yield, await, or return. If the parsing was a more intensive process, you might want to consider running this portion in its own process with loop.run_in_executor().Next, the coroutine write() takes a file object and a single URL, and waits on parse() to return a set of the parsed URLs, writing each to the file asynchronously along with its source URL through use of aiofiles, a package for async file IO.Lastly, bulk_crawl_and_write() serves as the main entry point into the script’s chain of coroutines. It uses a single session, and a task is created for each URL that is ultimately read from urls.txt.Here are a few additional points that deserve mention: The default ClientSession has an adapter with a maximum of 100 open connections. To change that, pass an instance of asyncio.connector.TCPConnector to ClientSession. You can also specify limits on a per-host basis. You can specify max timeouts for both the session as a whole and for individual requests. This script also uses async with, which works with an asynchronous context manager. I haven’t devoted a whole section to this concept because the transition from synchronous to asynchronous context managers is fairly straightforward. The latter has to define .__aenter__() and .__aexit__() rather than .__exit__() and .__enter__(). As you might expect, async with can only be used inside a coroutine function declared with async def.If you’d like to explore a bit more, the companion files for this tutorial up at GitHub have comments and docstrings attached as well.Here’s the execution in all of its glory, as areq.py gets, parses, and saves results for 9 URLs in under a second:$ python3 areq.py21:33:22 DEBUG:asyncio: Using selector: KqueueSelector21:33:22 INFO:areq: Got response [200] for URL: https://www.mediamatters.org/21:33:22 INFO:areq: Found 115 links for https://www.mediamatters.org/21:33:22 INFO:areq: Got response [200] for URL: https://www.nytimes.com/guides/21:33:22 INFO:areq: Got response [200] for URL: https://www.politico.com/tipsheets/morning-money21:33:22 INFO:areq: Got response [200] for URL: https://www.ietf.org/rfc/rfc2616.txt21:33:22 ERROR:areq: aiohttp exception for https://docs.python.org/3/this-url-will-404.html [404]: Not Found21:33:22 INFO:areq: Found 120 links for https://www.nytimes.com/guides/21:33:22 INFO:areq: Found 143 links for https://www.politico.com/tipsheets/morning-money21:33:22 INFO:areq: Wrote results for source URL: https://www.mediamatters.org/21:33:22 INFO:areq: Found 0 links for https://www.ietf.org/rfc/rfc2616.txt21:33:22 INFO:areq: Got response [200] for URL: https://1.1.1.1/21:33:22 INFO:areq: Wrote results for source URL: https://www.nytimes.com/guides/21:33:22 INFO:areq: Wrote results for source URL: https://www.politico.com/tipsheets/morning-money21:33:22 INFO:areq: Got response [200] for URL: https://www.bloomberg.com/markets/economics21:33:22 INFO:areq: Found 3 links for https://www.bloomberg.com/markets/economics21:33:22 INFO:areq: Wrote results for source URL: https://www.bloomberg.com/markets/economics21:33:23 INFO:areq: Found 36 links for https://1.1.1.1/21:33:23 INFO:areq: Got response [200] for URL: https://regex101.com/21:33:23 INFO:areq: Found 23 links for https://regex101.com/21:33:23 INFO:areq: Wrote results for source URL: https://regex101.com/21:33:23 INFO:areq: Wrote results for source URL: https://1.1.1.1/That’s not too shabby! As a sanity check, you can check the line-count on the output. In my case, it’s 626, though keep in mind this may fluctuate:$ wc -l foundurls.txt 626 foundurls.txt$ head -n 3 foundurls.txtsource_url parsed_urlhttps://www.bloomberg.com/markets/economics https://www.bloomberg.com/feedbackhttps://www.bloomberg.com/markets/economics https://www.bloomberg.com/notices/tos Next Steps: If you’d like to up the ante, make this webcrawler recursive. You can use aio-redis to keep track of which URLs have been crawled within the tree to avoid requesting them twice, and connect links with Python’s networkx library. Remember to be nice. Sending 1000 concurrent requests to a small, unsuspecting website is bad, bad, bad. There are ways to limit how many concurrent requests you’re making in one batch, such as in using the sempahore objects of asyncio or using a pattern like this one. If you don’t heed this warning, you may get a massive batch of TimeoutError exceptions and only end up hurting your own program.Async IO in ContextNow that you’ve seen a healthy dose of code, let’s step back for a minute and consider when async IO is an ideal option and how you can make the comparison to arrive at that conclusion or otherwise choose a different model of concurrency.When and Why Is Async IO the Right Choice?This tutorial is no place for an extended treatise on async IO versus threading versus multiprocessing. However, it’s useful to have an idea of when async IO is probably the best candidate of the three.The battle over async IO versus multiprocessing is not really a battle at all. In fact, they can be used in concert. If you have multiple, fairly uniform CPU-bound tasks (a great example is a grid search in libraries such as scikit-learn or keras), multiprocessing should be an obvious choice.Simply putting async before every function is a bad idea if all of the functions use blocking calls. (This can actually slow down your code.) But as mentioned previously, there are places where async IO and multiprocessing can live in harmony.The contest between async IO and threading is a little bit more direct. I mentioned in the introduction that “threading is hard.” The full story is that, even in cases where threading seems easy to implement, it can still lead to infamous impossible-to-trace bugs due to race conditions and memory usage, among other things.Threading also tends to scale less elegantly than async IO, because threads are a system resource with a finite availability. Creating thousands of threads will fail on many machines, and I don’t recommend trying it in the first place. Creating thousands of async IO tasks is completely feasible.Async IO shines when you have multiple IO-bound tasks where the tasks would otherwise be dominated by blocking IO-bound wait time, such as: Network IO, whether your program is the server or the client side Serverless designs, such as a peer-to-peer, multi-user network like a group chatroom Read/write operations where you want to mimic a “fire-and-forget” style but worry less about holding a lock on whatever you’re reading and writing toThe biggest reason not to use it is that await only supports a specific set of objects that define a specific set of methods. If you want to do async read operations with a certain DBMS, you’ll need to find not just a Python wrapper for that DBMS, but one that supports the async/await syntax. Coroutines that contain synchronous calls block other coroutines and tasks from running.For a shortlist of libraries that work with async/await, see the list at the end of this tutorial.Async IO It Is, but Which One?This tutorial focuses on async IO, the async/await syntax, and using asyncio for event-loop management and specifying tasks. asyncio certainly isn’t the only async IO library out there. This observation from Nathaniel J. Smith says a lot: [In] a few years, asyncio might find itself relegated to becoming one of those stdlib libraries that savvy developers avoid, like urllib2. … What I’m arguing, in effect, is that asyncio is a victim of its own success: when it was designed, it used the best approach possible; but since then, work inspired by asyncio – like the addition of async/await – has shifted the landscape so that we can do even better, and now asyncio is hamstrung by its earlier commitments. (Source)To that end, a few big-name alternatives that do what asyncio does, albeit with different APIs and different approaches, are curio and trio. Personally, I think that if you’re building a moderately sized, straightforward program, just using asyncio is plenty sufficient and understandable, and lets you avoid adding yet another large dependency outside of Python’s standard library.But by all means, check out curio and trio, and you might find that they get the same thing done in a way that’s more intuitive for you as the user. Many of the package-agnostic concepts presented here should permeate to alternative async IO packages as well.Odds and EndsIn these next few sections, you’ll cover some miscellaneous parts of asyncio and async/await that haven’t fit neatly into the tutorial thus far, but are still important for building and understanding a full program.Other Top-Level asyncio FunctionsIn addition to asyncio.run(), you’ve seen a few other package-level functions such as asyncio.create_task() and asyncio.gather().You can use create_task() to schedule the execution of a coroutine object, followed by asyncio.run():&gt;&gt;&gt; import asyncio&gt;&gt;&gt; async def coro(seq) -&gt; list:... \"\"\"'IO' wait time is proportional to the max element.\"\"\"... await asyncio.sleep(max(seq))... return list(reversed(seq))...&gt;&gt;&gt; async def main():... # This is a bit redundant in the case of one task... # We could use `await coro([3, 2, 1])` on its own... t = asyncio.create_task(coro([3, 2, 1])) # Python 3.7+... await t... print(f't: type {type(t)}')... print(f't done: {t.done()}')...&gt;&gt;&gt; t = asyncio.run(main())t: type &lt;class '_asyncio.Task'&gt;t done: TrueThere’s a subtlety to this pattern: if you don’t await t within main(), it may finish before main() itself signals that it is complete. Because asyncio.run(main()) calls loop.run_until_complete(main()), the event loop is only concerned (without await t present) that main() is done, not that the tasks that get created within main() are done. Without await t, the loop’s other tasks will be cancelled, possibly before they are completed. If you need to get a list of currently pending tasks, you can use asyncio.Task.all_tasks().Note: asyncio.create_task() was introduced in Python 3.7. In Python 3.6 or lower, use asyncio.ensure_future() in place of create_task().Separately, there’s asyncio.gather(). While it doesn’t do anything tremendously special, gather() is meant to neatly put a collection of coroutines (futures) into a single future. As a result, it returns a single future object, and, if you await asyncio.gather() and specify multiple tasks or coroutines, you’re waiting for all of them to be completed. (This somewhat parallels queue.join() from our earlier example.) The result of gather() will be a list of the results across the inputs:&gt;&gt;&gt; import time&gt;&gt;&gt; async def main():... t = asyncio.create_task(coro([3, 2, 1]))... t2 = asyncio.create_task(coro([10, 5, 0])) # Python 3.7+... print('Start:', time.strftime('%X'))... a = await asyncio.gather(t, t2)... print('End:', time.strftime('%X')) # Should be 10 seconds... print(f'Both tasks done: {all((t.done(), t2.done()))}')... return a...&gt;&gt;&gt; a = asyncio.run(main())Start: 16:20:11End: 16:20:21Both tasks done: True&gt;&gt;&gt; a[[1, 2, 3], [0, 5, 10]]You probably noticed that gather() waits on the entire result set of the Futures or coroutines that you pass it. Alternatively, you can loop over asyncio.as_completed() to get tasks as they are completed, in the order of completion. The function returns an iterator that yields tasks as they finish. Below, the result of coro([3, 2, 1]) will be available before coro([10, 5, 0]) is complete, which is not the case with gather():&gt;&gt;&gt; async def main():... t = asyncio.create_task(coro([3, 2, 1]))... t2 = asyncio.create_task(coro([10, 5, 0]))... print('Start:', time.strftime('%X'))... for res in asyncio.as_completed((t, t2)):... compl = await res... print(f'res: {compl} completed at {time.strftime(\"%X\")}')... print('End:', time.strftime('%X'))... print(f'Both tasks done: {all((t.done(), t2.done()))}')...&gt;&gt;&gt; a = asyncio.run(main())Start: 09:49:07res: [1, 2, 3] completed at 09:49:10res: [0, 5, 10] completed at 09:49:17End: 09:49:17Both tasks done: TrueLastly, you may also see asyncio.ensure_future(). You should rarely need it, because it’s a lower-level plumbing API and largely replaced by create_task(), which was introduced later.The Precedence of awaitWhile they behave somewhat similarly, the await keyword has significantly higher precedence than yield. This means that, because it is more tightly bound, there are a number of instances where you’d need parentheses in a yield from statement that are not required in an analogous await statement. For more information, see examples of await expressions from PEP 492.ConclusionYou’re now equipped to use async/await and the libraries built off of it. Here’s a recap of what you’ve covered: Asynchronous IO as a language-agnostic model and a way to effect concurrency by letting coroutines indirectly communicate with each other The specifics of Python’s new async and await keywords, used to mark and define coroutines asyncio, the Python package that provides the API to run and manage coroutinesResourcesPython Version SpecificsAsync IO in Python has evolved swiftly, and it can be hard to keep track of what came when. Here’s a list of Python minor-version changes and introductions related to asyncio: 3.3: The yield from expression allows for generator delegation. 3.4: asyncio was introduced in the Python standard library with provisional API status. 3.5: async and await became a part of the Python grammar, used to signify and wait on coroutines. They were not yet reserved keywords. (You could still define functions or variables named async and await.) 3.6: Asynchronous generators and asynchronous comprehensions were introduced. The API of asyncio was declared stable rather than provisional. 3.7: async and await became reserved keywords. (They cannot be used as identifiers.) They are intended to replace the asyncio.coroutine() decorator. asyncio.run() was introduced to the asyncio package, among a bunch of other features.If you want to be safe (and be able to use asyncio.run()), go with Python 3.7 or above to get the full set of features.ArticlesHere’s a curated list of additional resources: Real Python: Speed up your Python Program with Concurrency Real Python: What is the Python Global Interpreter Lock? CPython: The asyncio package source Python docs: Data model &gt; Coroutines TalkPython: Async Techniques and Examples in Python Brett Cannon: How the Heck Does Async-Await Work in Python 3.5? PYMOTW: asyncio A. Jesse Jiryu Davis and Guido van Rossum: A Web Crawler With asyncio Coroutines Andy Pearce: The State of Python Coroutines: yield from Nathaniel J. Smith: Some Thoughts on Asynchronous API Design in a Post-async/await World Armin Ronacher: I don’t understand Python’s Asyncio Andy Balaam: series on asyncio (4 posts) Stack Overflow: Python asyncio.semaphore in async-await function Yeray Diaz: AsyncIO for the Working Python Developer Asyncio Coroutine Patterns: Beyond await A few Python What’s New sections explain the motivation behind language changes in more detail: What’s New in Python 3.3 (yield from and PEP 380) What’s New in Python 3.6 (PEP 525 &amp; 530)From David Beazley: Generator: Tricks for Systems Programmers A Curious Course on Coroutines and Concurrency Generators: The Final FrontierYouTube talks: John Reese - Thinking Outside the GIL with AsyncIO and Multiprocessing - PyCon 2018 Keynote David Beazley - Topics of Interest (Python Asyncio) David Beazley - Python Concurrency From the Ground Up: LIVE! - PyCon 2015 Raymond Hettinger, Keynote on Concurrency, PyBay 2017 Thinking about Concurrency, Raymond Hettinger, Python core developer Miguel Grinberg Asynchronous Python for the Complete Beginner PyCon 2017 Yury Selivanov asyncawait and asyncio in Python 3 6 and beyond PyCon 2017 Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream What Is Async, How Does It Work, and When Should I Use It? (PyCon APAC 2014)Related PEPs PEP Date Created PEP 342 – Coroutines via Enhanced Generators 2005-05 PEP 380 – Syntax for Delegating to a Subgenerator 2009-02 PEP 3153 – Asynchronous IO support 2011-05 PEP 3156 – Asynchronous IO Support Rebooted: the “asyncio” Module 2012-12 PEP 492 – Coroutines with async and await syntax 2015-04 PEP 525 – Asynchronous Generators 2016-07 PEP 530 – Asynchronous Comprehensions 2016-09 Libraries That Work With async/awaitFrom aio-libs: aiohttp: Asynchronous HTTP client/server framework aioredis: Async IO Redis support aiopg: Async IO PostgreSQL support aiomcache: Async IO memcached client aiokafka: Async IO Kafka client aiozmq: Async IO ZeroMQ support aiojobs: Jobs scheduler for managing background tasks async_lru: Simple LRU cache for async IOFrom magicstack: uvloop: Ultra fast async IO event loop asyncpg: (Also very fast) async IO PostgreSQL supportFrom other hosts: trio: Friendlier asyncio intended to showcase a radically simpler design aiofiles: Async file IO asks: Async requests-like http library asyncio-redis: Async IO Redis support aioprocessing: Integrates multiprocessing module with asyncio umongo: Async IO MongoDB client unsync: Unsynchronize asyncio aiostream: Like itertools, but async" }, { "title": "强化学习第二版导图（一）：MDP、动态规划及蒙特卡洛法", "url": "/posts/RL-MindMap-1/", "categories": "学习, Reinforcement Learning", "tags": "强化学习, 思维导图", "date": "2021-09-11 22:15:05 +0800", "snippet": "本系列为 Sutton &amp; Barto -《Reinforcement Learning: An Introduction 2nd Edition》的个人读后思维导图记录，便于后续快速复习及回忆，本章对应原文的第1-5章。原书地址：incompleteideas.net，右键新标签查看图片原图。" }, { "title": "【转载】A Python Interpreter Written in Python", "url": "/posts/Repost-A-Python-Interpreter-Written-in-Python/", "categories": "", "tags": "Python, 转载", "date": "2021-07-19 05:39:10 +0800", "snippet": "本文为转载。非常有助于理解 Python 解释器的原理等，从此对各种 Python 插件、库等不在迷茫【比如 laike9m/Cyberbrain: Python debugging, redefined. (github.com)】。中文版由 qingyunha 翻译维护，原文地址为 500lines。原作者Allison是Dropbox的工程师，在那里她维护着世界上最大的由Python客户组成的网络。在Dropbox之前，她是Recurse Center的引导师, … 她在北美的PyCon做过关于Python内部机制的演讲，并且她喜欢奇怪的bugs。她的博客地址是akaptur.com.IntroductionByterun是一个用Python实现的Python解释器。随着我在Byterun上的工作，我惊讶并很高兴地的发现，这个Python解释器的基础结构可以满足500行的限制。在这一章我们会搞清楚这个解释器的结构，给你足够的知识探索下去。我们的目标不是向你展示解释器的每个细节—像编程和计算机科学其他有趣的领域一样，你可能会投入几年的时间去搞清楚这个主题。Byterun是Ned Batchelder和我完成的，建立在Paul Swartz的工作之上。它的结构和主要的Python实现（CPython）差不多，所以理解Byterun会帮助你理解大多数解释器特别是CPython解释器。（如果你不知道你用的是什么Python，那么很可能它就是CPython）。尽管Byterun很小，但它能执行大多数简单的Python程序。A Python Interpreter在开始之前，让我们缩小一下“Pyhton解释器”的意思。在讨论Python的时候，“解释器”这个词可以用在很多不同的地方。有的时候解释器指的是REPL，当你在命令行下敲下python时所得到的交互式环境。有时候人们会相互替代的使用Python解释器和Python来说明执行Python代码的这一过程。在本章，“解释器”有一个更精确的意思：执行Python程序过程中的最后一步。在解释器接手之前，Python会执行其他3个步骤：词法分析，语法解析和编译。这三步合起来把源代码转换成code object,它包含着解释器可以理解的指令。而解释器的工作就是解释code object中的指令。你可能很奇怪执行Python代码会有编译这一步。Python通常被称为解释型语言，就像Ruby，Perl一样，它们和编译型语言相对，比如C，Rust。然而，这里的术语并不是它看起来的那样精确。大多数解释型语言包括Python，确实会有编译这一步。而Python被称为解释型的原因是相对于编译型语言，它在编译这一步的工作相对较少（解释器做相对多的工作）。在这章后面你会看到，Python的编译器比C语言编译器需要更少的关于程序行为的信息。A Python Python InterpreterByterun是一个用Python写的Python解释器，这点可能让你感到奇怪，但没有比用C语言写C语言编译器更奇怪。（事实上，广泛使用的gcc编译器就是用C语言本身写的）你可以用几乎的任何语言写一个Python解释器。用Python写Python既有优点又有缺点。最大的缺点就是速度：用Byterun执行代码要比用CPython执行慢的多，CPython解释器是用C语言实现的并做了优化。然而Byterun是为了学习而设计的，所以速度对我们不重要。使用Python最大优点是我们可以仅仅实现解释器，而不用担心Python运行时的部分，特别是对象系统。比如当Byterun需要创建一个类时，它就会回退到“真正”的Python。另外一个优势是Byterun很容易理解，部分原因是它是用高级语言写的（Python！）（另外我们不会对解释器做优化 — 再一次，清晰和简单比速度更重要）Building an Interpreter在我们考察Byterun代码之前，我们需要一些对解释器结构的高层次视角。Python解释器是如何工作的？Python解释器是一个虚拟机,模拟真实计算机的软件。我们这个虚拟机是栈机器，它用几个栈来完成操作（与之相对的是寄存器机器，它从特定的内存地址读写数据）。Python解释器是一个字节码解释器：它的输入是一些命令集合称作字节码。当你写Python代码时，词法分析器，语法解析器和编译器生成code object让解释器去操作。每个code object都包含一个要被执行的指令集合 — 它就是字节码 — 另外还有一些解释器需要的信息。字节码是Python代码的一个中间层表示：它以一种解释器可以理解的方式来表示源代码。这和汇编语言作为C语言和机器语言的中间表示很类似。A Tiny Interpreter为了让说明更具体，让我们从一个非常小的解释器开始。它只能计算两个数的和，只能理解三个指令。它执行的所有代码只是这三个指令的不同组合。下面就是这三个指令： LOAD_VALUE ADD_TWO_VALUES PRINT_ANSWER我们不关心词法，语法和编译，所以我们也不在乎这些指令是如何产生的。你可以想象，你写下7 + 5，然后一个编译器为你生成那三个指令的组合。如果你有一个合适的编译器，你甚至可以用Lisp的语法来写，只要它能生成相同的指令。假设7 + 5生成这样的指令集：what_to_execute = { \"instructions\": [(\"LOAD_VALUE\", 0), # the first number (\"LOAD_VALUE\", 1), # the second number (\"ADD_TWO_VALUES\", None), (\"PRINT_ANSWER\", None)], \"numbers\": [7, 5] }Python解释器是一个栈机器，所以它必须通过操作栈来完成这个加法。(Figure 1.1)解释器先执行第一条指令，LOAD_VALUE，把第一个数压到栈中。接着它把第二个数也压到栈中。然后，第三条指令，ADD_TWO_VALUES,先把两个数从栈中弹出，加起来，再把结果压入栈中。最后一步，把结果弹出并输出。LOAD_VALUE这条指令告诉解释器把一个数压入栈中，但指令本身并没有指明这个数是多少。指令需要一个额外的信息告诉解释器去哪里找到这个数。所以我们的指令集有两个部分：指令本身和一个常量列表。（在Python中，字节码就是我们称为的“指令”，而解释器执行的是code object。）为什么不把数字直接嵌入指令之中？想象一下，如果我们加的不是数字，而是字符串。我们可不想把字符串这样的东西加到指令中，因为它可以有任意的长度。另外，我们这种设计也意味着我们只需要对象的一份拷贝，比如这个加法 7 + 7, 现在常量表 \"numbers\"只需包含一个7。你可能会想为什么会需要除了ADD_TWO_VALUES之外的指令。的确，对于我们两个数加法，这个例子是有点人为制作的意思。然而，这个指令却是建造更复杂程序的轮子。比如，就我们目前定义的三个指令，只要给出正确的指令组合，我们可以做三个数的加法，或者任意个数的加法。同时，栈提供了一个清晰的方法去跟踪解释器的状态，这为我们增长的复杂性提供了支持。现在让我们来完成我们的解释器。解释器对象需要一个栈，它可以用一个列表来表示。它还需要一个方法来描述怎样执行每条指令。比如，LOAD_VALUE会把一个值压入栈中。class Interpreter: def __init__(self): self.stack = [] def LOAD_VALUE(self, number): self.stack.append(number) def PRINT_ANSWER(self): answer = self.stack.pop() print(answer) def ADD_TWO_VALUES(self): first_num = self.stack.pop() second_num = self.stack.pop() total = first_num + second_num self.stack.append(total)这三个方法完成了解释器所理解的三条指令。但解释器还需要一样东西：一个能把所有东西结合在一起并执行的方法。这个方法就叫做run_code, 它把我们前面定义的字典结构what-to-execute作为参数，循环执行里面的每条指令，如何指令有参数，处理参数，然后调用解释器对象中相应的方法。 def run_code(self, what_to_execute): instructions = what_to_execute[\"instructions\"] numbers = what_to_execute[\"numbers\"] for each_step in instructions: instruction, argument = each_step if instruction == \"LOAD_VALUE\": number = numbers[argument] self.LOAD_VALUE(number) elif instruction == \"ADD_TWO_VALUES\": self.ADD_TWO_VALUES() elif instruction == \"PRINT_ANSWER\": self.PRINT_ANSWER()为了测试，我们创建一个解释器对象，然后用前面定义的 7 + 5 的指令集来调用run_code。 interpreter = Interpreter() interpreter.run_code(what_to_execute)显然，它会输出12尽管我们的解释器功能受限，但这个加法过程几乎和真正的Python解释器是一样的。这里，我们还有几点要注意。首先，一些指令需要参数。在真正的Python bytecode中，大概有一半的指令有参数。像我们的例子一样，参数和指令打包在一起。注意指令的参数和传递给对应方法的参数是不同的。第二，指令ADD_TWO_VALUES不需要任何参数，它从解释器栈中弹出所需的值。这正是以栈为基础的解释器的特点。记得我们说过只要给出合适的指令集，不需要对解释器做任何改变，我们做多个数的加法。考虑下面的指令集，你觉得会发生什么？如果你有一个合适的编译器，什么代码才能编译出下面的指令集？ what_to_execute = { \"instructions\": [(\"LOAD_VALUE\", 0), (\"LOAD_VALUE\", 1), (\"ADD_TWO_VALUES\", None), (\"LOAD_VALUE\", 2), (\"ADD_TWO_VALUES\", None), (\"PRINT_ANSWER\", None)], \"numbers\": [7, 5, 8] }从这点出发，我们开始看到这种结构的可扩展性：我们可以通过向解释器对象增加方法来描述更多的操作（只要有一个编译器能为我们生成组织良好的指令集）。Variables接下来给我们的解释器增加变量的支持。我们需要一个保存变量值的指令，STORE_NAME;一个取变量值的指令LOAD_NAME;和一个变量到值的映射关系。目前，我们会忽略命名空间和作用域，所以我们可以把变量和值的映射直接存储在解释器对象中。最后，我们要保证what_to_execute除了一个常量列表，还要有个变量名字的列表。&gt;&gt;&gt; def s():... a = 1... b = 2... print(a + b)# a friendly compiler transforms `s` into: what_to_execute = { \"instructions\": [(\"LOAD_VALUE\", 0), (\"STORE_NAME\", 0), (\"LOAD_VALUE\", 1), (\"STORE_NAME\", 1), (\"LOAD_NAME\", 0), (\"LOAD_NAME\", 1), (\"ADD_TWO_VALUES\", None), (\"PRINT_ANSWER\", None)], \"numbers\": [1, 2], \"names\": [\"a\", \"b\"] }我们的新的的实现在下面。为了跟踪哪名字绑定到那个值，我们在__init__方法中增加一个environment字典。我们也增加了STORE_NAME和LOAD_NAME方法，它们获得变量名，然后从environment字典中设置或取出这个变量值。现在指令参数就有两个不同的意思，它可能是numbers列表的索引，也可能是names列表的索引。解释器通过检查所执行的指令就能知道是那种参数。而我们打破这种逻辑 ，把指令和它所用何种参数的映射关系放在另一个单独的方法中。class Interpreter: def __init__(self): self.stack = [] self.environment = {} def STORE_NAME(self, name): val = self.stack.pop() self.environment[name] = val def LOAD_NAME(self, name): val = self.environment[name] self.stack.append(val) def parse_argument(self, instruction, argument, what_to_execute): \"\"\" Understand what the argument to each instruction means.\"\"\" numbers = [\"LOAD_VALUE\"] names = [\"LOAD_NAME\", \"STORE_NAME\"] if instruction in numbers: argument = what_to_execute[\"numbers\"][argument] elif instruction in names: argument = what_to_execute[\"names\"][argument] return argument def run_code(self, what_to_execute): instructions = what_to_execute[\"instructions\"] for each_step in instructions: instruction, argument = each_step argument = self.parse_argument(instruction, argument, what_to_execute) if instruction == \"LOAD_VALUE\": self.LOAD_VALUE(argument) elif instruction == \"ADD_TWO_VALUES\": self.ADD_TWO_VALUES() elif instruction == \"PRINT_ANSWER\": self.PRINT_ANSWER() elif instruction == \"STORE_NAME\": self.STORE_NAME(argument) elif instruction == \"LOAD_NAME\": self.LOAD_NAME(argument)仅仅五个指令，run_code这个方法已经开始变得冗长了。如果保持这种结构，那么每条指令都需要一个if分支。这里，我们要利用Python的动态方法查找。我们总会给一个称为FOO的指令定义一个名为FOO的方法，这样我们就可用Python的getattr函数在运行时动态查找方法，而不用这个大大的分支结构。run_code方法现在是这样： def execute(self, what_to_execute): instructions = what_to_execute[\"instructions\"] for each_step in instructions: instruction, argument = each_step argument = self.parse_argument(instruction, argument, what_to_execute) bytecode_method = getattr(self, instruction) if argument is None: bytecode_method() else: bytecode_method(argument)Real Python Bytecode现在，放弃我们的小指令集，去看看真正的Python字节码。字节码的结构和我们的小解释器的指令集差不多，除了字节码用一个字节而不是一个名字来指示这条指令。为了理解它的结构，我们将考察一个函数的字节码。考虑下面这个例子：&gt;&gt;&gt; def cond():... x = 3... if x &lt; 5:... return 'yes'... else:... return 'no'...Python在运行时会暴露一大批内部信息，并且我们可以通过REPL直接访问这些信息。对于函数对象cond，cond.__code__是与其关联的code object，而cond.__code__.co_code就是它的字节码。当你写Python代码时，你永远也不会想直接使用这些属性，但是这可以让我们做出各种恶作剧，同时也可以看看内部机制。&gt;&gt;&gt; cond.__code__.co_code # the bytecode as raw bytesb'd\\x01\\x00}\\x00\\x00|\\x00\\x00d\\x02\\x00k\\x00\\x00r\\x16\\x00d\\x03\\x00Sd\\x04\\x00Sd\\x00 \\x00S'&gt;&gt;&gt; list(cond.__code__.co_code) # the bytecode as numbers[100, 1, 0, 125, 0, 0, 124, 0, 0, 100, 2, 0, 107, 0, 0, 114, 22, 0, 100, 3, 0, 83, 100, 4, 0, 83, 100, 0, 0, 83]当我们直接输出这个字节码，它看起来完全无法理解 — 唯一我们了解的是它是一串字节。很幸运，我们有一个很强大的工具可以用：Python标准库中的dis module。dis是一个字节码反汇编器。反汇编器以为机器而写的底层代码作为输入，比如汇编代码和字节码，然后以人类可读的方式输出。当我们运行dis.dis, 它输出每个字节码的解释。&gt;&gt;&gt; dis.dis(cond) 2 0 LOAD_CONST 1 (3) 3 STORE_FAST 0 (x) 3 6 LOAD_FAST 0 (x) 9 LOAD_CONST 2 (5) 12 COMPARE_OP 0 (&lt;) 15 POP_JUMP_IF_FALSE 22 4 18 LOAD_CONST 3 ('yes') 21 RETURN_VALUE 6 &gt;&gt; 22 LOAD_CONST 4 ('no') 25 RETURN_VALUE 26 LOAD_CONST 0 (None) 29 RETURN_VALUE这些都是什么意思？让我们以第一条指令LOAD_CONST为例子。第一列的数字（2）表示对应源代码的行数。第二列的数字是字节码的索引，告诉我们指令LOAD_CONST在0位置。第三列是指令本身对应的人类可读的名字。如果第四列存在，它表示指令的参数。如果第5列存在，它是一个关于参数是什么的提示。考虑这个字节码的前几个字节：[100, 1, 0, 125, 0, 0]。这6个字节表示两条带参数的指令。我们可以使用dis.opname，一个字节到可读字符串的映射，来找到指令100和指令125代表是什么：&gt;&gt;&gt; dis.opname[100]'LOAD_CONST'&gt;&gt;&gt; dis.opname[125]'STORE_FAST'第二和第三个字节 — 1 ，0 —是LOAD_CONST的参数，第五和第六个字节 — 0，0 — 是STORE_FAST的参数。就像我们前面的小例子，LOAD_CONST需要知道的到哪去找常量，STORE_FAST需要找到名字。（Python的LOAD_CONST和我们小例子中的LOAD_VALUE一样，LOAD_FAST和LOAD_NAME一样）。所以这六个字节代表第一行源代码x = 3.(为什么用两个字节表示指令的参数？如果Python使用一个字节，每个code object你只能有256个常量/名字，而用两个字节，就增加到了256的平方，65536个）。Conditionals and Loops到目前为止，我们的解释器只能一条接着一条的执行指令。这有个问题，我们经常会想多次执行某个指令，或者在特定的条件下略过它们。为了可以写循环和分支结构，解释器必须能够在指令中跳转。在某种程度上，Python在字节码中使用GOTO语句来处理循环和分支！让我们再看一个cond函数的反汇编结果：&gt;&gt;&gt; dis.dis(cond) 2 0 LOAD_CONST 1 (3) 3 STORE_FAST 0 (x) 3 6 LOAD_FAST 0 (x) 9 LOAD_CONST 2 (5) 12 COMPARE_OP 0 (&lt;) 15 POP_JUMP_IF_FALSE 22 4 18 LOAD_CONST 3 ('yes') 21 RETURN_VALUE 6 &gt;&gt; 22 LOAD_CONST 4 ('no') 25 RETURN_VALUE 26 LOAD_CONST 0 (None) 29 RETURN_VALUE第三行的条件表达式if x &lt; 5被编译成四条指令：LOAD_FAST, LOAD_CONST, COMPARE_OP和 POP_JUMP_IF_FALSE。x &lt; 5对应加载x，加载5，比较这两个值。指令POP_JUMP_IF_FALSE完成if语句。这条指令把栈顶的值弹出，如果值为真，什么都不发生。如果值为假，解释器会跳转到另一条指令。这条将被加载的指令称为跳转目标，它作为指令POP_JUMP的参数。这里，跳转目标是22，索引为22的指令是LOAD_CONST,对应源码的第6行。（dis用&gt;&gt;标记跳转目标。）如果X &lt; 5为假，解释器会忽略第四行（return yes）,直接跳转到第6行（return \"no\"）。因此解释器通过跳转指令选择性的执行指令。Python的循环也依赖于跳转。在下面的字节码中，while x &lt; 5这一行产生了和if x &lt; 10几乎一样的字节码。在这两种情况下，解释器都是先执行比较，然后执行POP_JUMP_IF_FALSE来控制下一条执行哪个指令。第四行的最后一条字节码JUMP_ABSOLUT(循环体结束的地方），让解释器返回到循环开始的第9条指令处。当 x &lt; 10变为假，POP_JUMP_IF_FALSE会让解释器跳到循环的终止处，第34条指令。&gt;&gt;&gt; def loop():... x = 1... while x &lt; 5:... x = x + 1... return x...&gt;&gt;&gt; dis.dis(loop) 2 0 LOAD_CONST 1 (1) 3 STORE_FAST 0 (x) 3 6 SETUP_LOOP 26 (to 35) &gt;&gt; 9 LOAD_FAST 0 (x) 12 LOAD_CONST 2 (5) 15 COMPARE_OP 0 (&lt;) 18 POP_JUMP_IF_FALSE 34 4 21 LOAD_FAST 0 (x) 24 LOAD_CONST 1 (1) 27 BINARY_ADD 28 STORE_FAST 0 (x) 31 JUMP_ABSOLUTE 9 &gt;&gt; 34 POP_BLOCK 5 &gt;&gt; 35 LOAD_FAST 0 (x) 38 RETURN_VALUEExplore Bytecode我鼓励你用dis.dis来试试你自己写的函数。一些有趣的问题值得探索： 对解释器而言for循环和while循环有什么不同？ 能不能写出两个不同函数，却能产生相同的字节码? elif是怎么工作的？列表推导呢？Frames到目前为止，我们已经知道了Python虚拟机是一个栈机器。它能顺序执行指令，在指令间跳转，压入或弹出栈值。但是这和我们心想的解释器还有一定距离。在前面的那个例子中，最后一条指令是RETURN_VALUE,它和return语句想对应。但是它返回到哪里去呢？为了回答这个问题，我们必须严增加一层复杂性：frame。一个frame是一些信息的集合和代码的执行上下文。frames在Python代码执行时动态的创建和销毁。每个frame对应函数的一次调用。— 所以每个frame只有一个code object与之关联，而一个code object可以很多frame。比如你有一个函数递归的调用自己10次，这时有11个frame。总的来说，Python程序的每个作用域有一个frame，比如，每个module，每个函数调用，每个类定义。Frame存在于调用栈中，一个和我们之前讨论的完全不同的栈。（你最熟悉的栈就是调用栈，就是你经常看到的异常回溯，每个以”File ‘program.py’“开始的回溯对应一个frame。）解释器在执行字节码时操作的栈，我们叫它数据栈。其实还有第三个栈，叫做块栈，用于特定的控制流块，比如循环和异常处理。调用栈中的每个frame都有它自己的数据栈和块栈。让我们用一个具体的例子来说明。假设Python解释器执行到标记为3的地方。解释器正在foo函数的调用中，它接着调用bar。下面是frame调用栈，块栈和数据栈的示意图。我们感兴趣的是解释器先从最底下的foo()开始，接着执行foo的函数体，然后到达bar。&gt;&gt;&gt; def bar(y):... z = y + 3 # &lt;--- (3) ... and the interpreter is here.... return z...&gt;&gt;&gt; def foo():... a = 1... b = 2... return a + bar(b) # &lt;--- (2) ... which is returning a call to bar ......&gt;&gt;&gt; foo() # &lt;--- (1) We're in the middle of a call to foo ...3现在，解释器在bar函数的调用中。调用栈中有3个fram：一个对应于module层，一个对应函数foo,别一个对应函数bar。(Figure 1.2)一旦bar返回，与它对应的frame就会从调用栈中弹出并丢弃。字节码指令RETURN_VALUE告诉解释器在frame间传递一个值。首先，它把位于调用栈栈顶的frame中的数据栈的栈顶值弹出。然后把整个frame弹出丢弃。最后把这个值压到下一个frame的数据栈中。当Ned Batchelder和我在写Byterun时，很长一段时间我们的实现中一直有个重大的错误。我们整个虚拟机中只有一个数据栈，而不是每个frame都有个一个。我们做了很多测试，同时在Byterun和真正的Python上，为了保证结果一致。我们几乎通过了所有测试，只有一样东西不能通过，那就是生成器。最后，通过仔细的阅读Cpython的源码，我们发现了错误所在。把数据栈移到每个frame就解决了这个问题。回头在看看这个bug，我惊讶的发现Python真的很少依赖于每个frame有一个数据栈这个特性。在Python中几乎所有的操作都会清空数据栈，所以所有的frame公用一个数据栈是没问题的。在上面的例子中，当bar执行完后，它的数据栈为空。即使foo公用这一个栈，它的值也不会受影响。然而，对应生成器，一个关键的特点是它能暂停一个frame的执行，返回到其他的frame，一段时间后它能返回到原来的frame，并以它离开时的同样的状态继续执行。Byterun现在我们有足够的Python解释器的知识背景去考察Byterun。Byterun中有四种对象。 VirtualMachine类，它管理高层结构，frame调用栈，指令到操作的映射。这是一个比前面Inteprter对象更复杂的版本。 Frame类，每个Frame类都有一个code object，并且管理者其他一些必要的状态信息，全局和局部命名空间，指向调用它的frame的指针和最后执行的字节码指令。 Function类，它被用来代替真正的Python函数。回想一下，调用函数时会创建一个新的frame。我们自己实现Function，所以我们控制新frame的创建。 Block类，它只是包装了代码块的3个属性。（代码块的细节不是解释器的核心，我们不会花时间在它身上，把它列在这里，是因为Byterun需要它。）The VirtualMachine Class程序运行时只有一个VirtualMachine被创建，因为我们只有一个解释器。VirtualMachine保存调用栈，异常状态，在frame中传递的返回值。它的入口点是run_code方法，它以编译后的code object为参数，以创建一个frame为开始，然后运行这个frame。这个frame可能再创建出新的frame；调用栈随着程序的运行增长缩短。当第一个frame返回时，执行结束。class VirtualMachineError(Exception): passclass VirtualMachine(object): def __init__(self): self.frames = [] # The call stack of frames. self.frame = None # The current frame. self.return_value = None self.last_exception = None def run_code(self, code, global_names=None, local_names=None): \"\"\" An entry point to execute code using the virtual machine.\"\"\" frame = self.make_frame(code, global_names=global_names, local_names=local_names) self.run_frame(frame)The Frame Class接下来，我们来写Frame对象。frame是一个属性的集合，它没有任何方法。前面提到过，这些属性包括由编译器生成的code object；局部，全局和内置命名空间；前一个frame的引用；一个数据栈；一个块栈；最后执行的指令。（对于内置命名空间我们需要多做一点工作，Python对待这个命名空间不同；但这个细节对我们的虚拟机不重要。）class Frame(object): def __init__(self, code_obj, global_names, local_names, prev_frame): self.code_obj = code_obj self.global_names = global_names self.local_names = local_names self.prev_frame = prev_frame self.stack = [] if prev_frame: self.builtin_names = prev_frame.builtin_names else: self.builtin_names = local_names['__builtins__'] if hasattr(self.builtin_names, '__dict__'): self.builtin_names = self.builtin_names.__dict__ self.last_instruction = 0 self.block_stack = []接着，我们在虚拟机中增加对frame的操作。这有3个帮助函数：一个创建新的frame的方法，和压栈和出栈的方法。第四个函数，run_frame,完成执行frame的主要工作，待会我们再讨论这个方法。class VirtualMachine(object): [... snip ...] # Frame manipulation def make_frame(self, code, callargs={}, global_names=None, local_names=None): if global_names is not None and local_names is not None: local_names = global_names elif self.frames: global_names = self.frame.global_names local_names = {} else: global_names = local_names = { '__builtins__': __builtins__, '__name__': '__main__', '__doc__': None, '__package__': None, } local_names.update(callargs) frame = Frame(code, global_names, local_names, self.frame) return frame def push_frame(self, frame): self.frames.append(frame) self.frame = frame def pop_frame(self): self.frames.pop() if self.frames: self.frame = self.frames[-1] else: self.frame = None def run_frame(self): pass # we'll come back to this shortlyThe Function ClassFunction的实现有点扭曲，但是大部分的细节对理解解释器不重要。重要的是当调用函数时 — __call__方法被调用 — 它创建一个新的Frame并运行它。class Function(object): \"\"\" Create a realistic function object, defining the things the interpreter expects. \"\"\" __slots__ = [ 'func_code', 'func_name', 'func_defaults', 'func_globals', 'func_locals', 'func_dict', 'func_closure', '__name__', '__dict__', '__doc__', '_vm', '_func', ] def __init__(self, name, code, globs, defaults, closure, vm): \"\"\"You don't need to follow this closely to understand the interpreter.\"\"\" self._vm = vm self.func_code = code self.func_name = self.__name__ = name or code.co_name self.func_defaults = tuple(defaults) self.func_globals = globs self.func_locals = self._vm.frame.f_locals self.__dict__ = {} self.func_closure = closure self.__doc__ = code.co_consts[0] if code.co_consts else None # Sometimes, we need a real Python function. This is for that. kw = { 'argdefs': self.func_defaults, } if closure: kw['closure'] = tuple(make_cell(0) for _ in closure) self._func = types.FunctionType(code, globs, **kw) def __call__(self, *args, **kwargs): \"\"\"When calling a Function, make a new frame and run it.\"\"\" callargs = inspect.getcallargs(self._func, *args, **kwargs) # Use callargs to provide a mapping of arguments: values to pass into the new # frame. frame = self._vm.make_frame( self.func_code, callargs, self.func_globals, {} ) return self._vm.run_frame(frame)def make_cell(value): \"\"\"Create a real Python closure and grab a cell.\"\"\" # Thanks to Alex Gaynor for help with this bit of twistiness. fn = (lambda x: lambda: x)(value) return fn.__closure__[0]接着，回到VirtualMachine对象，我们对数据栈的操作也增加一些帮助方法。字节码操作的栈总是在当前frame的数据栈。这让我们能完成POP_TOP,LOAD_FAST,并且让其他操作栈的指令可读性更高。class VirtualMachine(object): [... snip ...] # Data stack manipulation def top(self): return self.frame.stack[-1] def pop(self): return self.frame.stack.pop() def push(self, *vals): self.frame.stack.extend(vals) def popn(self, n): \"\"\"Pop a number of values from the value stack. A list of `n` values is returned, the deepest value first. \"\"\" if n: ret = self.frame.stack[-n:] self.frame.stack[-n:] = [] return ret else: return []在我们运行frame之前，我们还需两个方法。第一个方法，parse_byte_and_args,以一个字节码为输入，先检查它是否有参数，如果有，就解析它的参数。这个方法同时也更新frame的last_instruction属性，它指向最后执行的指令。一条没有参数的指令只有一个字节长度，而有参数的字节有3个字节长。参数的意义依赖于指令是什么。比如，前面说过，指令POP_JUMP_IF_FALSE,它的参数指的是跳转目标。BUILD_LIST, 它的参数是列表的个数。LOAD_CONST,它的参数是常量的索引。一些指令用简单的数字作为参数。对于另一些，虚拟机需要一点努力去发现它意义。标准库中的dismodule中有一个备忘单解释什么参数有什么意思，这让我们的代码更加简洁。比如，列表dis.hasname告诉我们LOAD_NAME, IMPORT_NAME,LOAD_GLOBAL,和另外的9个指令都有同样的意思：名字列表的索引。class VirtualMachine(object): [... snip ...] def parse_byte_and_args(self): f = self.frame opoffset = f.last_instruction byteCode = f.code_obj.co_code[opoffset] f.last_instruction += 1 byte_name = dis.opname[byteCode] if byteCode &gt;= dis.HAVE_ARGUMENT: # index into the bytecode arg = f.code_obj.co_code[f.last_instruction:f.last_instruction+2] f.last_instruction += 2 # advance the instruction pointer arg_val = arg[0] + (arg[1] * 256) if byteCode in dis.hasconst: # Look up a constant arg = f.code_obj.co_consts[arg_val] elif byteCode in dis.hasname: # Look up a name arg = f.code_obj.co_names[arg_val] elif byteCode in dis.haslocal: # Look up a local name arg = f.code_obj.co_varnames[arg_val] elif byteCode in dis.hasjrel: # Calculate a relative jump arg = f.last_instruction + arg_val else: arg = arg_val argument = [arg] else: argument = [] return byte_name, argument下一个方法是dispatch,它查看给定的指令并执行相应的操作。在CPython中，这个分派函数用一个巨大的switch语句完成，有超过1500行的代码。幸运的是，我们用的是Python，我们的代码会简洁的多。我们会为每一个字节码名字定义一个方法，然后用getattr来查找。就像我们前面的小解释器一样，如果一条指令叫做FOO_BAR，那么它对应的方法就是byte_FOO_BAR。现在，我们先把这些方法当做一个黑盒子。每个指令方法都会返回None或者一个字符串why,有些情况下虚拟机需要这个额外why信息。这些指令方法的返回值，仅作为解释器状态的内部指示，千万不要和执行frame的返回值相混淆。class VirtualMachine(object): [... snip ...] def dispatch(self, byte_name, argument): \"\"\" Dispatch by bytename to the corresponding methods. Exceptions are caught and set on the virtual machine.\"\"\" # When later unwinding the block stack, # we need to keep track of why we are doing it. why = None try: bytecode_fn = getattr(self, 'byte_%s' % byte_name, None) if bytecode_fn is None: if byte_name.startswith('UNARY_'): self.unaryOperator(byte_name[6:]) elif byte_name.startswith('BINARY_'): self.binaryOperator(byte_name[7:]) else: raise VirtualMachineError( \"unsupported bytecode type: %s\" % byte_name ) else: why = bytecode_fn(*argument) except: # deal with exceptions encountered while executing the op. self.last_exception = sys.exc_info()[:2] + (None,) why = 'exception' return why def run_frame(self, frame): \"\"\"Run a frame until it returns (somehow). Exceptions are raised, the return value is returned. \"\"\" self.push_frame(frame) while True: byte_name, arguments = self.parse_byte_and_args() why = self.dispatch(byte_name, arguments) # Deal with any block management we need to do while why and frame.block_stack: why = self.manage_block_stack(why) if why: break self.pop_frame() if why == 'exception': exc, val, tb = self.last_exception e = exc(val) e.__traceback__ = tb raise e return self.return_valueThe Block Class在我们完成每个字节码方法前，我们简单的讨论一下块。一个块被用于某种控制流，特别是异常处理和循环。它负责保证当操作完成后数据栈处于正确的状态。比如，在一个循环中，一个特殊的迭代器会存在栈中，当循环完成时它从栈中弹出。解释器需要检查循环仍在继续还是已经停止。为了跟踪这些额外的信息，解释器设置了一个标志来指示它的状态。我们用一个变量why实现这个标志，它可以None或者是下面几个字符串这一，\"continue\", \"break\",\"excption\",return。他们指示对块栈和数据栈进行什么操作。回到我们迭代器的例子，如果块栈的栈顶是一个loop块，why是continue,迭代器就因该保存在数据栈上，不是如果why是break,迭代器就会被弹出。块操作的细节比较精密，我们不会花时间在这上面，但是有兴趣的读者值得仔细的看看。Block = collections.namedtuple(\"Block\", \"type, handler, stack_height\")class VirtualMachine(object): [... snip ...] # Block stack manipulation def push_block(self, b_type, handler=None): level = len(self.frame.stack) self.frame.block_stack.append(Block(b_type, handler, stack_height)) def pop_block(self): return self.frame.block_stack.pop() def unwind_block(self, block): \"\"\"Unwind the values on the data stack corresponding to a given block.\"\"\" if block.type == 'except-handler': # The exception itself is on the stack as type, value, and traceback. offset = 3 else: offset = 0 while len(self.frame.stack) &gt; block.level + offset: self.pop() if block.type == 'except-handler': traceback, value, exctype = self.popn(3) self.last_exception = exctype, value, traceback def manage_block_stack(self, why): \"\"\" \"\"\" frame = self.frame block = frame.block_stack[-1] if block.type == 'loop' and why == 'continue': self.jump(self.return_value) why = None return why self.pop_block() self.unwind_block(block) if block.type == 'loop' and why == 'break': why = None self.jump(block.handler) return why if (block.type in ['setup-except', 'finally'] and why == 'exception'): self.push_block('except-handler') exctype, value, tb = self.last_exception self.push(tb, value, exctype) self.push(tb, value, exctype) # yes, twice why = None self.jump(block.handler) return why elif block.type == 'finally': if why in ('return', 'continue'): self.push(self.return_value) self.push(why) why = None self.jump(block.handler) return why return whyThe Instructions剩下了的就是完成那些指令方法了：byte_LOAD_FAST,byte_BINARY_MODULO等等。而这些指令的实现并不是很有趣，这里我们只展示了一小部分，完整的实现在这儿（足够执行我们前面所述的所有代码了。）class VirtualMachine(object): [... snip ...] ## Stack manipulation def byte_LOAD_CONST(self, const): self.push(const) def byte_POP_TOP(self): self.pop() ## Names def byte_LOAD_NAME(self, name): frame = self.frame if name in frame.f_locals: val = frame.f_locals[name] elif name in frame.f_globals: val = frame.f_globals[name] elif name in frame.f_builtins: val = frame.f_builtins[name] else: raise NameError(\"name '%s' is not defined\" % name) self.push(val) def byte_STORE_NAME(self, name): self.frame.f_locals[name] = self.pop() def byte_LOAD_FAST(self, name): if name in self.frame.f_locals: val = self.frame.f_locals[name] else: raise UnboundLocalError( \"local variable '%s' referenced before assignment\" % name ) self.push(val) def byte_STORE_FAST(self, name): self.frame.f_locals[name] = self.pop() def byte_LOAD_GLOBAL(self, name): f = self.frame if name in f.f_globals: val = f.f_globals[name] elif name in f.f_builtins: val = f.f_builtins[name] else: raise NameError(\"global name '%s' is not defined\" % name) self.push(val) ## Operators BINARY_OPERATORS = { 'POWER': pow, 'MULTIPLY': operator.mul, 'FLOOR_DIVIDE': operator.floordiv, 'TRUE_DIVIDE': operator.truediv, 'MODULO': operator.mod, 'ADD': operator.add, 'SUBTRACT': operator.sub, 'SUBSCR': operator.getitem, 'LSHIFT': operator.lshift, 'RSHIFT': operator.rshift, 'AND': operator.and_, 'XOR': operator.xor, 'OR': operator.or_, } def binaryOperator(self, op): x, y = self.popn(2) self.push(self.BINARY_OPERATORS[op](x, y)) COMPARE_OPERATORS = [ operator.lt, operator.le, operator.eq, operator.ne, operator.gt, operator.ge, lambda x, y: x in y, lambda x, y: x not in y, lambda x, y: x is y, lambda x, y: x is not y, lambda x, y: issubclass(x, Exception) and issubclass(x, y), ] def byte_COMPARE_OP(self, opnum): x, y = self.popn(2) self.push(self.COMPARE_OPERATORS[opnum](x, y)) ## Attributes and indexing def byte_LOAD_ATTR(self, attr): obj = self.pop() val = getattr(obj, attr) self.push(val) def byte_STORE_ATTR(self, name): val, obj = self.popn(2) setattr(obj, name, val) ## Building def byte_BUILD_LIST(self, count): elts = self.popn(count) self.push(elts) def byte_BUILD_MAP(self, size): self.push({}) def byte_STORE_MAP(self): the_map, val, key = self.popn(3) the_map[key] = val self.push(the_map) def byte_LIST_APPEND(self, count): val = self.pop() the_list = self.frame.stack[-count] # peek the_list.append(val) ## Jumps def byte_JUMP_FORWARD(self, jump): self.jump(jump) def byte_JUMP_ABSOLUTE(self, jump): self.jump(jump) def byte_POP_JUMP_IF_TRUE(self, jump): val = self.pop() if val: self.jump(jump) def byte_POP_JUMP_IF_FALSE(self, jump): val = self.pop() if not val: self.jump(jump) ## Blocks def byte_SETUP_LOOP(self, dest): self.push_block('loop', dest) def byte_GET_ITER(self): self.push(iter(self.pop())) def byte_FOR_ITER(self, jump): iterobj = self.top() try: v = next(iterobj) self.push(v) except StopIteration: self.pop() self.jump(jump) def byte_BREAK_LOOP(self): return 'break' def byte_POP_BLOCK(self): self.pop_block() ## Functions def byte_MAKE_FUNCTION(self, argc): name = self.pop() code = self.pop() defaults = self.popn(argc) globs = self.frame.f_globals fn = Function(name, code, globs, defaults, None, self) self.push(fn) def byte_CALL_FUNCTION(self, arg): lenKw, lenPos = divmod(arg, 256) # KWargs not supported here posargs = self.popn(lenPos) func = self.pop() frame = self.frame retval = func(*posargs) self.push(retval) def byte_RETURN_VALUE(self): self.return_value = self.pop() return \"return\"Dynamic Typing: What the Compiler Doesn’t Know你可能听过Python是一种动态语言 — 是它是动态类型的。在我们建造解释器的过程中，已经流露出这个描述。动态的一个意思是很多工作在运行时完成。前面我们看到Python的编译器没有很多关于代码真正做什么的信息。举个例子，考虑下面这个简单的函数mod。它区两个参数，返回它们的模运算值。从它的字节码中，我们看到变量a和b首先被加载，然后字节码BINAY_MODULO完成这个模运算。&gt;&gt;&gt; def mod(a, b):... return a % b&gt;&gt;&gt; dis.dis(mod) 2 0 LOAD_FAST 0 (a) 3 LOAD_FAST 1 (b) 6 BINARY_MODULO 7 RETURN_VALUE&gt;&gt;&gt; mod(19, 5)4计算19 % 5得4，— 一点也不奇怪。如果我们用不同类的参数呢？&gt;&gt;&gt; mod(\"by%sde\", \"teco\")'bytecode'刚才发生了什么？你可能见过这样的语法，格式化字符串。&gt;&gt;&gt; print(\"by%sde\" % \"teco\")bytecode用符号%去格式化字符串会调用字节码BUNARY_MODULO.它取栈顶的两个值求模，不管这两个值是字符串，数字或是你自己定义的类的实例。字节码在函数编译时生成（或者说，函数定义时）相同的字节码会用于不同类的参数。Python的编译器关于字节码的功能知道的很少。而取决于解释器来决定BINAYR_MODULO应用于什么类型的对象并完成真确的操作。这就是为什么Python被描述为动态类型：直到你运行前你不必知道这个函数参数的类型。相反，在一个静态类型语言中，程序员需要告诉编译器参数的类型是什么（或者编译器自己推断出参数的类型。）编译器的无知是优化Python的一个挑战 — 只看字节码，而不真正运行它，你就不知道每条字节码在干什么！你可以定义一个类，实现__mod__方法，当你对这个类的实例使用%时，Python就会自动调用这个方法。所以，BINARY_MODULO其实可以运行任何代码。看看下面的代码，第一个a % b看起来没有用。def mod(a,b): a % b return a %b不幸的是，对这段代码进行静态分析 — 不运行它 — 不能确定第一个a % b没有做任何事。用 %调用__mod__可能会写一个文件，或是和程序的其他部分交互，或者其他任何可以在Python中完成的事。很难优化一个你不知道它会做什么的函数。在Russell Power和Alex Rubinsteyn的优秀论文中写道，“我们可用多快的速度解释Python？”，他们说，“在普遍缺乏类型信息下，每条指令必须被看作一个INVOKE_ARBITRARY_METHOD。”ConclusionByterun是一个比CPython容易理解的简洁的Python解释器。Byterun复制了CPython的主要结构：一个基于栈的指令集称为字节码，它们顺序执行或在指令间跳转，向栈中压入和从中弹出数据。解释器随着函数和生成器的调用和返回，动态的创建，销毁frame，并在frame间跳转。Byterun也有着和真正解释器一样的限制：因为Python使用动态类型，解释器必须在运行时决定指令的正确行为。我鼓励你去反汇编你的程序，然后用Byterun来运行。你很快会发现这个缩短版的Byterun所没有实现的指令。完整的实现在这里或者仔细阅读真正的CPython解释器ceval.c,你也可以实现自己的解释器！AcknowledgementsThanks to Ned Batchelder for originating this project and guiding my contributions, Michael Arntzenius for his help debugging the code and editing the prose, Leta Montopoli for her edits, and the entire Recurse Center community for their support and interest. Any errors are my own.A Python Interpreter Written in Python is maintained by qingyunha.This page was generated by GitHub Pages using the Cayman theme by Jason Long." }, { "title": "常用软件记录", "url": "/posts/DailySoftware/", "categories": "", "tags": "MacOS, 软件", "date": "2021-06-28 00:29:28 +0800", "snippet": "记录一下这几年中黑苹果常用的一些软件以备后用。总的来说：花钱买服务，使用体验最好的大部分情况下都是收费软件，再钱包鼓鼓的情况下直接使用收费软件是一般情况下的最优解。工具MindMaster：亿图思维导图 地址：亿图脑图MindMaster 多平台思维导图软件，让您的创意破茧而出 (edrawsoft.cn) 价格：收费 平台：Mac，IOS，Windows，Linux跨平台、支持在线编辑、完整的 LaTeX 支持等等，且支持一次性买断并提供后续更新，避免月供宰羊。文字Texpad：LaTeX编辑器 地址：Texpad · Smoothest way to write LaTeX 价格：收费 平台：Mac，IOS，Windows非常棒的本地 LaTeX 编辑器，有着实时预览，提示，自动补全等特性。界面美观简洁，开箱即用。Overleaf：在线LaTeX论文协作 地址：https://www.overleaf.com/ 价格：免费+ 平台：网页版功能没有 Texpad 强大，但是主打多人在线协作功能，支持多人评论、回复，修改评审、历史记录等等协作专用功能。缺点是不容易处理大文件，如果是想写毕业论文之类的，还是推荐本地，不然在线编译容易超时出错等。推荐用该工具进行论文的协作修改，其它情况就算了。Typora：优秀的markdown编辑器 地址：Typora — a markdown editor, markdown reader. 价格：免费 平台：Mac，Linux，Windows没啥好多说的，在单文件编辑上该软件可以称作顶级，不具备文件组织、管理功能。作为默认的 markdown 文件打开软件是最佳的。Notion：好用的笔记软件 地址：Notion – The all-in-one workspace for your notes, tasks, wikis, and databases. 价格：免费+ 平台：Mac，Windows，Android，IOS，非官方 Linux 版本 缺点 ：Notion 员工能够看到你的所有文档！ 之前是用印象笔记的，但是后面印象笔记更新的功能越来越用不到，一些小细节地方又不修改，就换过来了。Notion Enhancer：让 Notion 变得更好用官方版本缺乏很多功能，比如悬浮文章目录，多标签页打开等，主题也很缺乏。目前没看到官方有增加这些功能的意愿。 地址：https://github.com/notion-enhancer/notion-enhancer 价格：免费 平台：Mac，Windows，LinuxJoplin：隐私、自建笔记支持加密、自建同步服务器等，注重隐私的不二之选。 地址：https://joplinapp.org/ 价格：免费，官方同步云收费，但是可以自建同步服务器 平台：Mac，Windows，Linux" }, { "title": "Jellyfin: 个人媒体中心搭建", "url": "/posts/Jellyfin-Tutorial/", "categories": "", "tags": "Linux, 娱乐", "date": "2021-06-11 22:40:09 +0800", "snippet": " Jellyfin是一个开源免费的媒体解决方案：让你能够完美控制你的媒体文件。它能够实现从个人服务器，没有任何附加条件地串流至任何设备。Your media，your server，your way。可以把Jellyfin视作你个人的爱奇艺等视频网站！和传统文件夹保存视频、影片等相比，有着以下优点： 更好的组织你的媒体文件：能够从 IMDB 等影片信息网站匹配视频相关信息并展现出来，包括但不限于：字幕下载，海报横幅下载，演员信息拉取，电影简介、电视剧季集信息等获取等。 任意设备浏览器即可观看：无需集中在一台设备上观看，在能够访问到个人 Jellyfin 服务器的任何地方都能够通过浏览器观看视频。 多用户：每个用户有着不同的资料库访问权限，各自的观看进度均独立保存。 支持不同类型的影音文件：电影、电视剧、音乐以及在线电视节目。 官方在线 demo：Jellyfin截图：\t \t Jellyfin 部署Jellyfin 能够直接通过包管理器直接安装，官方也提供了docker容器镜像，因此 Jellyfin 可以运行在任何具备 docker 环境的地方，比如一台 VPS 或者群辉设备等。受限于设备限制，这里只展示在 VPS 上借助 docker 部署 Jellyfin 的方法。VPS 选取具体的配置要求取决于 Jellyfin 的设置：服务端解码还是客户端解码？视频质量（分辨率等）高低？同时在线观看的人数？具体情况还是需要根据自己的视频资源测试后才能有个大概，这里仅供参考，提醒一下 VPS 的配置对于视频观看的观感有着很大影响。这里没有做过具体的实验，从个人使用体验来看，如果在服务端解码，最好能够由 GPU 进行硬件加速，单靠 CPU 解码会消耗大量资源，尤其是 4k 视频下，CPU 解码时拖动视频进度条无法及时响应，会卡顿几秒，甚至无法流畅播放，GPU 解码就好多了。网络方面，Jellyfin 几档默认配置：1080p - 10~60 M，4k - 80~120 M（取决于视频的最高质量）。Jellyfin 容器部署 这里请直接参考官方文档：[Installing Jellyfin Documentation - Jellyfin Project](https://jellyfin.org/docs/general/administration/installing.html) 。总体流程分为以下几步： 安装 docker 环境：[Install Docker Engine Docker Documentation](https://docs.docker.com/engine/install/) 配置 docker 环境下的硬件加速【可选】：[Hardware Acceleration Documentation - Jellyfin Project](https://jellyfin.org/docs/general/administration/hardware-acceleration.html) 配置并部署 Jellyfin 容器这里给出个人使用的 docker 部署命令，其中： 3-7 行为 NVIDIA 硬件加速所需的参数，见上面第 2 点 10-13 行将 VPS 的存储空间挂载到容器中以实现数据存储，防止每次重启容器的数据丢失。其中： /config 为 Jellyfin 的配置相关目录，将 /data/jellyfin/.jellyfin/config 修改为任意你想使用的目录 /cache 为 Jellyfin 的缓存目录，将 /data/jellyfin/.jellyfin/cache 修改为任意你想使用的目录 /media 为 Jellyfin 的媒体目录，里面应该存放你的视频、音乐等文件，将 /data/jellyfin 修改为你想使用的目录 默认端口为 8096，或者通过 -p port:8096 参数自定义 docker run -d \\ --name jellyfin \\ -e NVIDIA_DRIVER_CAPABILITIES=all \\ -e NVIDIA_VISIBLE_DEVICES=all \\ --gpus all \\ --device /dev/dri/renderD128:/dev/dri/renderD128 \\ --device /dev/dri/card0:/dev/dri/card0 \\ --user 0:0 \\ --net=host \\ --volume /data/jellyfin/.jellyfin/config:/config \\ --volume /data/jellyfin/.jellyfin/cache:/cache \\ --mount type=bind,source=/data/jellyfin,target=/media \\ --restart=unless-stopped \\ jellyfin/jellyfinJellyfin 可能存在的问题 无法刮削视频的元数据：一般都是因为网络原因，导致无法访问 IMDB 等元数据网站，或者访问超时。虽然可以通过合理设置代理的方式解决，但是鉴于 Jellyfin 本身在元数据管理、修改方面较弱，不建议使用代理方法。请参考下一节 TinyMediaManager 方法。 无法及时扫描文件夹的变化：在人为修改元数据后，主动触发重新扫描，有时发现修改后的元数据并没有起效。该问题亦是由于网络原因导致，无法联网获取元数据导致了无法及时刷新（即便人为修改了）。这种情况需要在媒体库设置中，将自动刮削元数据选项取消勾选。 字幕不显示：字幕加载了，但是不显示。一般是由于字幕的字体缺失。通过 vim 之类的看一下字幕里面是不是指定了字体，然后在 Jellyfin 设置里，勾选候补字体，然后选择候补字体文件夹，再把缺失字体放进去即可。视频元数据刮削：TinyMediaManagerTinyMediaManager 是一个基于 Java 的媒体管理工具，能够为 Kodi，MediaPortal 以及 Plex 等从 TheMovieDB，Imdb，Ofdb 等网站刮削媒体元数据。相较于 Jellyfin 自带的功能，该工具提供了更丰富的刮削、修改功能，且操作界面更加直观。官网见：tinyMediaManager ，截图如下：感谢社区，让 TinyMediaManager 能够以容器的方式跑在服务器上，通过浏览器就能够使用。这里使用 dzhuang/tinymediamanager-docker: A repository for creating a docker container including TinyMediaManager with GUI interface. (github.com) 版本，主要加入了中文字体支持。部署命令如下，主要参数： 端口为 8095，将 -p 8095:5800 修改为想使用的端口即可。 配置文件夹 /config ，将 /data/jellyfin/.tmm/config 修改为想使用的目录即可。该目录与 Jellyfin 没有关联，可以任意选取可用目录。 媒体文件夹 /media ，将 /data/jellyfin 修改为想使用的目录即可。该目录与 Jellyfin 中的媒体目录需要保持一致！docker run -p 8095:5800 --user 0:0 -v /data/jellyfin/.tmm/config:/config -v /data/jellyfin/:/media dzhuang/tinymediamanager之后在设置中配置好 电影、剧 各自的目录，然后点击更新源 按钮就可看到自己的视频文件，右键任意资源即可进行一系列操作。网络问题可以在设置中配置代理解决。 TinyMediaManager 需要注意媒体文件夹的读写权限！否则会失败。视频下载组合：Frostwire + TransmissionFrostwire 是一个提供了 BT 搜索的下载客户端，这里借用其搜索功能寻找视频资源，官网：FrostWire - BitTorrent Client, Cloud Downloader, Media Player. 100% Free Download, No subscriptions required.Transmission 是一个能够在服务器上运行的 BT 下载客户端，通过网页即可管理，见教程：How to set up transmission-daemon on a Raspberry Pi and control it via web interface - LinuxConfig.org\t \t\t 基本使用流程： Frostwire 上搜索你想要的视频 右键复制 magent 信息 在 Transmission 中下载 TinyMediaManager 刮削 Jellyfin 观看突破内网限制：Zerotier/n2n/SoftEther这里不再介绍这三款软件。考虑到 Zerotier 能够以最高成功率实现跨 Nat 直连而无需自己的中转服务器，建议使用 Zerotier 以节省流量。请参考以下博客： Zerotier：用ZeroTier搭建属于自己的虚拟局域网(VLAN) - 知乎 (zhihu.com) SoftEther：使用 SoftEther VPN 搭建虚拟局域网 - 米V米 (mivm.cn) n2n：n2n实现内网穿透 - 简书 (jianshu.com)SoftEther 自启动新建 /etc/init.d/vpnclient ，内容如下：#!/bin/sh# Start/stop the vpnclien daemon#### BEGIN INIT INFO# Provides: vpnclient# Required-Start: $local_fs $syslog $time# Required-Stop: $local_fs $syslog $time# Should-Start: $network# Should-Stop: $network# Default-Start: 2 3 4 5# Default-Stop:# Short-Description: vpn client service# Description: Softether vpn client service.### END INIT INFOEXE_DIR=/opt/vpnclient/SER=\"$EXE_DIR\"vpnclientCMD=\"$EXE_DIR\"vpncmdstart(){ echo start $SER start $CMD localhost /client /cmd accountconnect 【你的连接名称。如果该连接设置了自启动，那么就不需要了】 ifconfig 【你的vpn的网络适配器名称】 【你的ip】 netmask 255.0.0.0 echo end}stop(){ $SER stop}case \"$1\" instart)start;;stop)stop;;esacexit 0注意还需要给它执行权限：sudo chmod +x /etc/init.d/vpnclient然后交由 systemd 管理：sudo ln -s /etc/init.d/vpnclient /etc/rc5.d/S99vpnclientsudo systemctl enable vpnclientsudo systemctl start vpnclient之后即可开机自启。" }, { "title": "Linux 桌面上的全局菜单实现原理", "url": "/posts/Linux-Global-Menu-Principle/", "categories": "", "tags": "Linux", "date": "2021-06-09 06:09:18 +0800", "snippet": " 全局菜单指的是将原本属于应用窗口内部的菜单栏（一般位于标题栏下面）脱离窗口独立各自的应用窗口显示，且没有导致原有的功能缺失。macOS 上面就有相当完善的全局菜单机制，Linux上最近几年各大桌面环境诸如 KDE、Gnome 也陆续支持全局菜单（KDE是有官方支持的，Gnome则是第三方插件实现的）。并没有考证过全局菜单在 Linux 上的发展历史，但是从 KDE 实现全局菜单的原理来看，使用到了 com.canonical.AppMenu.Registrar.xml 以及 com.canonical.dbusmenu.xml 两个 DBus 描述文件，从这两个文件名字来看，应该是基于 unity 的遗留财产了。这里只介绍 KDE 上的全局菜单原理。基本原理简单来说，就是将应用程序内部的菜单栏，通过序列化的方式暴露到标准的 DBus 上，然后由全局菜单服务从对应的 DBus 中反序列化回菜单栏并显示出来。所以要做的事情有： 不同类型的应用程序将自己的菜单栏序列化暴露到 DBus 上：这一步显然得是框架支持，Qt,GTK之类的 通过 DBus 反序列化得到菜单，并根据当前窗口的 id 显示对应的菜单栏先说序列化与反序列化菜单的情况：这个其实已经有现成实现得了，直接拿来用就好了，appmenu-qt,appmenu-gtk-module 这些，所以这一块就不用操心了。其次是怎么样才能知道每个程序全局菜单的 DBus? 显然需要框架主动告诉系统，该程序的 DBus service name 是什么，path 是什么，这样才有可能成功获取到菜单。在这件事情上，Qt 和 Gtk 的处理方法是不同的。 Qt首先 Qt 本身已经支持全局菜单了，参见 Qt文档。当 com.canonical.AppMenu.Registrar DBus 存在的时候，程序就会自动向该 DBus 注册，那只需要在实现这个 DBus Service 的时候，记录下来 Qt 上报的内容就能够知道该 Qt 程序的 DBus 服务名称以及路径，再结合窗口 ID，就很容易定位到 Gtkgtk 本身是不支持全局菜单的，需要借助 GTK_MODULES=appmenu-gtk-module 才能够实现。当使用了该 module 后，它也会检测 com.canonical.AppMenu.Registrar DBus 是否存在，如果存在就启动全局菜单。 但是不同的是，Gtk 并不会向这个 Dbus 注册程序的 DBus 服务名称和路径，而是将这些信息写在了窗口属性里，需要使用 xlib 之类的进行对应属性的获取。下面是几个比较关键的属性名称，可以参照 gtk wiki 查看。 _GTK_APPLICATION_OBJECT_PATH _GTK_WINDOW_OBJECT_PATH _GTK_MENUBAR_OBJECT_PATH _GTK_APP_MENU_OBJECT_PATH那么思路就很清楚了，Qt 程序的话，就通过 com.canonical.AppMenu.Registrar DBUS 读取程序的全局菜单的 dbus 服务名称和路径，GTK 的话，就通过窗口属性。KDE 原理KDE 为 Qt 和 Gtk 做了统一，最终将程序的 dbus 名称和路径写在了窗口属性 _KDE_NET_WM_APPMENU_SERVICE_NAME 以及 _KDE_NET_WM_APPMENU_OBJECT_PATH。首先是Qt，具体代码请参考 plasma-workspace/appmenu。它实现了 com.canonical.AppMenu.Registrar DBus 服务，并在窗口注册实现里，将全局菜单的 dbus 服务名称和路径记录在了窗口属性中。序列化的方式是 dbusmenu其次是 Gtk，由于 Gtk 采用的是另外一套序列化工具，所以 KDE 做了一个代理，见 gmenu-dbusmenu-proxy。这里面做了两件事，一是读取 Gtk 窗口的属性并记录到 _KDE_NET_WM_APPMENU_SERVICE_NAME 以及 _KDE_NET_WM_APPMENU_OBJECT_PATH；二是将 Gtk 序列化的方式转为了 dbusmenu 方式。最后就是全局菜单的插件了，监控窗口变化事件，DBus 上的菜单更新事件，为每个窗口呈现各自的全局菜单，见 plasma-active-window-controlDeepin OS 上的全局菜单实现具体见： SeptemberHX/dde-top-panel: dde top panel for Deepin V20 (github.com) ：负责监控 DBus 并从 DBus 中读取并显示出当前活跃窗口的菜单 SeptemberHX/dde-globalmenu-service: DBus service registery for dde-globalmenu (github.com) ：负责启动 DBus 并将不同的全局菜单实现（Qt、Gtk等）做一层统一代理以方便统一调用" } ]
